---
title: "Designing and Understanding Field Experiments: Resources from the EGAP Learning Days"
author: Jake Bowers^[[University of Illinois @ Urbana-Champaign](https://jakebowers.org)], Maarten Voors^[[Wageningen University](https://sites.google.com/site/maartenvoors/)], Nahomi Ichino^[[Emory University](https://nahomi.github.io/)]
date: '`r format(Sys.Date(), "%B %d, %Y")`'
site: bookdown::bookdown_site
knit: "bookdown::render_book"
documentclass: book
bibliography: learningdays-guide.bib
biblio-style: apalike
link-citations: yes
colorlinks: yes
lot: yes
lof: yes
github-repo: egap/learningdays-guide
description: "EGAP Learning Days, causal inference, randomized experiments, field experiments, experimental design, research design"
fontsize: 12pt
geometry: margin=1in
graphics: yes
---
```{r include=FALSE, cache=FALSE}
## Currently empty.

#require(Hmisc)    # provides knitrSet and other functions
#knitrSet(lang='markdown', fig.path='png/', fig.align='left', w=6.5, h=4.5, cache=FALSE)
# If using blogdown: knitrSet(lang='blogdown')

#`r hidingTOC(buttonLabel="Outline")`

```

```{r echo=FALSE, include=FALSE, message=FALSE, warning=FALSE}
require(knitr)
opts_chunk$set(
  strip.white = TRUE,
  width.cutoff = 132,
  size = "\\scriptsize",
  out.width = ".9\\textwidth",
  message = FALSE,
  warning = FALSE,
  echo = TRUE,
  comment = NA,
  tidy = "styler",
  prompt = FALSE,
  results = "markup"
)

library(here)
library(DeclareDesign)
library(randomizr)
library(estimatr)
library(lmtest)
library(sandwich)
library(coin) ## for easier randomization inference
library(blockTools)
library(quickblock)
library(tidyverse)
library(kableExtra)
library(future)
library(future.apply)

## install.packages("remotes")
## remotes::install_github("gibbonscharlie/bfe")
## library(bfe) ## for another approach to block randomized trials

options(
  htmltools.dir.version = FALSE, formatR.indent = 2,
  width = 132, digits = 4, warnPartialMatchAttr = FALSE, warnPartialMatchDollar = FALSE,
  repos = "https://cloud.r-project.org"
)

# local({
#  r = getOption('repos')
#  if (!length(r) || identical(unname(r['CRAN']), '@CRAN@'))
#    r['CRAN'] = 'https://cran.rstudio.com'
#  options(repos = r)
# })

# lapply(c('DT', 'citr', 'formatR', 'svglite'), function(pkg) {
#  if (system.file(package = pkg) == '') install.packages(pkg)
# })

# RItools version with balanceTest
## Run this only once and then not again until we want a new version from github
## Leaving this as a comment for now. It should be handled by renv more easily.
## library('devtools')
## library('withr')
## library('here')
## if(!dir.exists(here::here('libraries'))){
## 	dir.create(here::here('libraries'))
## }
##
## withr::with_libpaths(new = here::here('libraries'), remotes::install_github("markmfredrickson/RItools",ref="randomization-distribution",force=TRUE),'prefix')
library("RItools")

## https://tex.stackexchange.com/questions/148188/knitr-xcolor-incompatible-color-definition/254482
knit_hooks$set(document = function(x) {
  sub("\\usepackage[]{color}", "\\usepackage{xcolor}", x, fixed = TRUE)
})
```


```{r htmlTemp3, echo=FALSE, eval=TRUE}
## This next from https://stackoverflow.com/questions/45360998/code-folding-in-bookdown
codejs <- readr::read_lines("js/codefolding.js")
collapsejs <- readr::read_lines("js/collapse.js")
transitionjs <- readr::read_lines("js/transition.js")

## Default to showing code
## window.initializeCodeFolding("show" === "show");
## Default to hiding code
## window.initializeCodeFolding("show" === "show");



# https://stackoverflow.com/questions/43009788/insert-a-logo-in-upper-right-corner-of-r-markdown-html-document
img <- htmltools::img(
  src = knitr::image_uri("Resources/Images/egap-logo.png"),
  alt = "logo",
  style = "position:relative; top:50px; right:1%; padding:10px;z-index:200;"
)
# style = 'position:absolute; top:50px; right:1%; padding:10px;z-index:200;')

## htmlhead <- paste0('
## <script>
## document.write(\'<div class="logos">',img,'</div>\')
## </script>
## ')

## See also https://stackoverflow.com/questions/38333691/r-markdown-putting-an-image-in-the-top-right-hand-corner-of-html-and-moving-tit

htmlhead <-
  paste("
        <script>",
    paste(transitionjs, collapse = "\n"),
    "</script>
        <script>",
    paste(collapsejs, collapse = "\n"),
    "</script>
<script>",
    paste(codejs, collapse = "\n"),
    '</script>
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
.row { display: flex; }
.collapse { display: none; }
.in { display:block }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "hide");
});
</script>
',
    sep = "\n"
  )


readr::write_lines(htmlhead, file = "header.html")
```

# Introduction

Over the past decade, [Evidence in Governance and Politics (EGAP)](http://egap.org/) has organized [Learning Days](https://egap.org/learning-days/) workshops on experimental methods for principal investigators (PIs) in Africa and Latin America with the aim of building experimental social-science research capacity among researchers and practitioners.  By sharing rigorous research methods with workshop participants, the Learning Days hopes to identify and build researcher networks around the world and to create strong, productive connections between EGAP members and those researchers.

The Learnings Days workshops are a combination of design clinics, research presentations, guided work with statistical software, and topical lectures by a small group of instructors, largely professors and PhD students from the EGAP Network.  The workshops focus on methods for the design and analysis of randomized experiments in the field rather than on randomized experiments in the lab or non-randomized studies.

**This book** grew out of a desire to share the materials we developed for the Learning Days.  The current version is written primarily for **instructors and organizers** of similar workshops and courses aimed at principal investigators (PIs) --- i.e., professors, post-doctoral fellows, PhD students, and NGO/government agency evaluators --- who implement experimental research projects on programs related to institutions, governance, and development.  Much of the material will also be useful as a refresher for past participants of the Learning Days workshops. 

This book is a comprehensive overview of causal inference methods for researchers developing an experimental research design.  It is organized in **modules** and covers topics such causal inference, randomization, hypothesis testing, estimands, estimators, statistical power, threats to inference, and the ethics of experimentation. The modules appear in the order in which the Learning Days instructors have found most useful to present the material.  However, the modules are linked to one another and can be reordered to suit your needs as an instructor.  In the appendix, we include some course preliminaries including a [glossary of terms](glossary-of-terms.html), an overview of basic statistics, and an introduction to R and RStudio.

The book includes **slides** on the core content, the [EGAP research design form](researchdesignform.html), and **references** to research examples and previous Learning Days materials.  The material included here builds significantly on and links to EGAP's work on methodology, summarized in the [EGAP Methods Guides](https://egap.org/methods-guides/).  We have also expanded on the material that is usually covered during the Learning Days workshops: the slides and modules presented here contain too much information to be covered in a single week. We present more rather than less information here, however, as an aid to instructors tailoring a local course to a specific audience.


## How to use the book

In order to most benefit from the book, please have [R](https://cran.r-project.org/) and [RStudio](https://www.rstudio.com/products/rstudio/download/) installed on your machine. In fact, the slides assume you will use R+markdown to personalize them for your own purposes.

To get going with R, see [Getting Started in R and Simple Statistics](Resources/Exercises/r-exercise-gettingstarted.Rmd).

You can copy this book or parts thereof (e.g., slides, etc.) either by using the Download button on the front page of <http://github.com/egap/learningdays-guide> OR by using github directly (by forking this repository).

We are happy for anyone to use the materials as long as EGAP is attributed. See Creative Commons Attribution-ShareAlike 4.0 International License for the precise terms.

## We love to hear from you!

If you have any questions, feedback or have organized your own event, please get in touch! Simply post an issue on [Github](https://github.com/egap/learningdays-guide/issues) or contact us via email, admin@egap.org.

## Acknowledgments

The materials included in this book have been developed over the past years by various Learning Days instructors. These include (in alphabetical order) Jake Bowers, Jasper Cooper, Ana De la O, Lindsay Dolan, Natalia Garbiras Díaz, Macartan Humphreys, Nahomi Ichino, Salif Jaiteh, Gareth Nellis, Dan Nielson, Rafael Piñeiro, Fernando Rosenblatt, Tara Slough, Peter van der Windt and Maarten Voors. At EGAP tremendous support has been provided by Matt Lisiecki, Ingrid Lee, Goldie Negelev, Max Mendez-Back and others. Learning Days have been generously funded by the Hewlett Foundation and supported by institutions around the world such as the  African School of Economics (Benin), Universidad Diego Portales (Chile), Universidad de los Andes (Colombia), Ghana Center for Democratic Development (Ghana), Mercy Corps (Guatemala), Invest in Knowledge (Malawi), NYU Abu Dhabi (UAE), Universidad Catolica del Uruguay (Uruguay).

<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
## Currently empty.

#require(Hmisc)    # provides knitrSet and other functions
#knitrSet(lang='markdown', fig.path='png/', fig.align='left', w=6.5, h=4.5, cache=FALSE)
# If using blogdown: knitrSet(lang='blogdown')

#`r hidingTOC(buttonLabel="Outline")`

```
# The Research Design Processes

This book aims to help you design and field randomized field experiments. And the modules that follow dive into the details of causal inference and statistics that will help you make wise decisions about a given experiment. However, an experiment that is well designed from a statistical sense may not answer the right question, or it may address that question poorly. And, even if the experiment does answer a good question well, policy-makers and scholars may not believe the results if the analysis of the data or fieldwork is difficult for them to understand.

In this module, we introduce the [EGAP Research Design
Form](Resources/Exercises/design-form.Rmd) that we have created as a kind of checklist to guide you through the stages in the research process. We also point toward the  [DeclareDesign](http://declaredesign.org) software package which enables us to explore the consequences of different research design decisions. We will use this form for the rest of the course.

Finally, we use this module as a space to talk about pre-analysis planning and registration. When plan our analyses and make these plans public, we improve our chances of persuading others with our results. See for one policy example the white paper on [Preregistration as a Tool for Strengthening Federal Evaluation](https://oes.gsa.gov/assets/files/preregistration-as-a-tool-in-federal-evaluation.pdf) from the US Government's Office of Evaluation Sciences (you can also see examples of their pre-analysis plans on all of [their field experiment pages](https://oes.gsa.gov/work/)).

We cannot provide an easy recipe for finding and articulating a good
scientific or policy question, we hope here to provide some space for discussion of these topics so that they are not ignored.


## Core Content

- What makes a **good research question**? A good research question advances science and/or is a question the answer to which will change a policy decision.

- Certain **research designs** are better able to address certain questions. We want to choose the design that best answers our key questions within our constraints.

- The questions we ask arise, often implicitely, from our values and from our understandings about how the world works. These **theories** structure our questions --- make the questions relevant. And the experiments that we execute teach us about the theory --- we hope that the evidence and data arising from these research designs improves our understanding.

- What are the **core parts of a research design**?

- Introduce core components of the [EGAP Research Design Form](Resources/Exercises/design-form.Rmd). 

- Introduce a research design software package, [DeclareDesign](http://declaredesign.org).

- The move in social science towards the **review of designs, rather than outcomes**.

 - **Pre-registration**: what is it? why should we do it? how should we do it? (EGAP Methods Guide [10 Things to Know about Pre-Analysis Plans](https://egap.org/resource/10-things-to-know-about-pre-analysis-plans/) )


## Slides

Below are slides with the core content that we cover in our lecture on research desuign. You can directly use these slides or make your own local copy and edit.

- [R Markdown Source](Resources/Slides/researchdesignform-slides.Rmd)

- [PDF Version](Resources/Slides/researchdesignform-slides.pdf)

- [HTML Version](Resources/Slides/researchdesignform-slides.html)

You can also see the slides used in previous EGAP Learning Days:

 - [The DeclareDesign presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018](Resources/Slides/Examples/declare_design-montevideo.pdf)

 - [The DeclareDesign presentation from EGAP Learning Days in Salima, Malawi, February 2017](Resources/Slides/Examples/declare_design-malawi.pdf)

 - [The DeclareDesign presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016](Resources/Slides/Examples/declare_design-santiago.pdf)

You can also see slides for Design Talks in previous EGAP Learning Days, where presenters focus on issues that come up in designing the research, rather than the results:

 - [Design Talk from EGAP Learning Days at African School of Economics, Benin, March 2018](Resources/Slides/Examples/research_design_2-benin.pdf)

 - [Design Talk from EGAP Learning Days in Salima, Malawi, February 2017](Resources/Slides/Examples/research_design-malawi.pdf)

 - [Design Talk 1 from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018](Resources/Slides/Examples/research_design_1-montevideo.pdf)

 - [Design Talk 2 from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018](Resources/Slides/Examples/research_design_2-montevideo.pdf)

 - [Design Talk 3 from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018](Resources/Slides/Examples/research_design_3-montevideo.pdf)

 - [Design Talk 1 from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016](Resources/Slides/Examples/research_design_1-santiago.pdf)

 - [Design Talk 2 from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016](Resources/Slides/Examples/research_design_2-santiago.pdf)

 - [Design Talk 3 from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016](Resources/Slides/Examples/research_design_3-santiago.pdf)

 - [Design Talk from EGAP Learning Days in Guatemala City, Guatemala, August 2017](Resources/Slides/Examples/research_design-guatemala.pdf)

## Design Form and Pre-Registration

- [Research Design Form](Resources/Exercises/design-form.Rmd)

- Links to repositories for pre-registration/pre-analysis plans:
    - EGAP registry, hosted by OSF (https://egap.org/registry/)
    - AEA RCT registry (https://www.socialscienceregistry.org/)
    - OSF (https://osf.io/registries)

- Examples of other pre-registrations/pre-analysis plans:
    - SMS Messages in Mozambique from the [US Federal Government](https://oes.gsa.gov/projects/sms-mozambique/)
    - Police Body-Cameras from the [Lab @ DC](https://osf.io/472zh)

## Resources

### EGAP Methods Guides

- EGAP Methods Guide [10 Things to Know about Pre-Analysis Plans](https://egap.org/resource/10-things-to-know-about-pre-analysis-plans/)
- EGAP Methods Guide [10 Things to Know about Measurement in Experiments](https://egap.org/resource/10-things-to-know-about-measurement-in-experiments/)


### Books, Chapters, and Articles

- [Christensen, Garret S., Jeremy Freese, and Edward Miguel. 2019. Transparent and Reproducible Social Science Research: How to Do Open Science. Oakland, California: University of California Press.
](@christensen_transparent_2019). A great book that summarizes new approaches in social science research on transparency and reproducibility.

### Tools

- [DeclareDesign](https://declaredesign.org/), an exciting and comprehensive set of software tools for describing, assessing, and conducting empirical research.


<!--chapter:end:researchdesignform.Rmd-->

```{r include=FALSE, cache=FALSE}
## Currently empty.

#require(Hmisc)    # provides knitrSet and other functions
#knitrSet(lang='markdown', fig.path='png/', fig.align='left', w=6.5, h=4.5, cache=FALSE)
# If using blogdown: knitrSet(lang='blogdown')

#`r hidingTOC(buttonLabel="Outline")`

```
# Causal Inference {.tabset}

Much of social science is about causality: we may ask if say [personal narratives of immigrants help reduce prejudicial attitudes towards them](https://egap.org/resource/brief-70-how-personal-narratives-reduce-negative-attitudes-towards-immigrants-in-kenya/), or whether [voter registration increases political participation](https://egap.org/resource/electoral-administration-in-kenya/) or if [bottom-up accountability can improve health outcomes](https://egap.org/resource/does-bottom-up-accountability-work-evidence-from-uganda/).

Even though we make causal claims all the time, not all are based on good comparisons. A good reseach design helps us establish whether there is a causal effect of a policy, action or program on a particular outcome. There is a long history of work on causality dating back to classic writing of Fisher (1935) and Rubin (1974). Over the past decade, with the increased use of experiments,  social science has become a much more serious how such claims are made. Randomization has been embraced as the gold standard. 

In this module, we introduce the counterfactual approach to causal inference, and these causal claims can be interpreted. We introduce the potential outcome framework and introduce how random assignment helps us make claims about what would have happened without the policy, action or program we study. We then introduce the three core assumptions of any causal claim: randomization, non-interference and excludability. 

## Core content

 - Introduce what do we mean when we say **"cause"**? (And why does it matter to be clear about the meaning of causal claims?)

 - An introduction to **potential outcomes** as a way to think about alternative states of the world.

 - Show how **randomization** helps us learn about counterfactual causal claims in a particularly useful way.

 - Introduce the three key **core assumptions** for causal inference: random assignment of subjects to treatment, non-interference, excludability.

 - Compare randomized versus observational studies.

 - Randomization brings high internal validity, but it can't promise **external validity**.

 - Your causal question closely links so your [Research Design](research-design.html).

## Slides

Below are slides with the core content that we cover in our lecture on causality. You can directly use these slides or make your own local copy and edit.

- [R Markdown Source](Resources/Slides/causalinference-slides.Rmd)

- [PDF Version](Resources/Slides/causalinference-slides.pdf)

- [HTML Version](Resources/Slides/causalinference-slides.html)

You can also see the slides used in previous EGAP Learning Days:

 - [The causal inference presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019](Resources/Slides/Examples/causality-benin.pdf)

 - [The causal inference presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018](Resources/Slides/Examples/causality-montevideo.pdf)

 - [The causal inference presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019](Resources/Slides/Examples/causality-bogota.pdf)

 - [The causal inference presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017](Resources/Slides/Examples/causality-guatemala.pdf)

 - [The introduction to experiments presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017](Resources/Slides/Examples/intro_experiments-guatemala.pdf)

 - [The causal inference presentation from EGAP Learning Days in Salima, Malawi, February 2017](Resources/Slides/Examples/causality-malawi.pdf)

 - [The causal inference presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016](Resources/Slides/Examples/causality-santiago.pdf)

## Resources

### EGAP Methods Guides

- EGAP Methods Guide [10 Things You Need to Know about Causal Inference](https://egap.org/resource/10-things-to-know-about-causal-inference/)

- EGAP Methods Guide [10 Strategies for Figuring Out If X Caused Y](https://egap.org/resource/10-strategies-figuring-out-if-x-caused-y/)

- EGAP Methods Guide [10 Things You Need to Know about Mechanisms](https://egap.org/resource/10-things-mechanisms/)

- EGAP Methods Guide [10 Things to Know About External Validity](https://egap.org/resource/10-things-to-know-about-external-validity/)

### Books, Chapters, and Articles

- "Causation and Explanation in Social Science" [@brady2008causation]

- Field Experiments: Design, Analysis, and Interpretation, Chapter 1 [@gerber_field_2012].  This book is a great resource for many topics in experimental design.

- Counterfactuals and Causal Inference: Methods and Principles for Social Research, Chapter 1 [@morgan_counterfactuals_2007].  This book includes nice examples of thinking through making causal claims from observational data.

- Running Randomized Evaluations: A Practical Guide [@glennerster_running_2013].  This is a great introduction with many examples of field studies for policy-makers.

### EGAP Policy Briefs

Some examples of causal questions:

- [Does bottom-up, citizen-based monitoring improve public service delivery?](https://egap.org/resource/brief-69-bottom-up-accountability-and-public-service-provision-in-brazil/)

- [Can bottom-up accountability generate improvements in health outcomes?](https://egap.org/resource/does-bottom-up-accountability-work-evidence-from-uganda/)

- [Can free and anonymous information communication technology strengthen local accountability and improve the delivery of public services?](https://egap.org/resource/does-information-technology-improve-public-service-delivery-lessons-from-uganda/)

- [Are radio voter education campaigns effective in discouraging voters from voting for parties/candidates that engage in vote-buying?](https://egap.org/resource/brief-38-diminishing-the-effectiveness-of-vote-buying-through-voter-education/)

<!--chapter:end:causalinference.Rmd-->

```{r include=FALSE, cache=FALSE}
## Currently empty.

#require(Hmisc)    # provides knitrSet and other functions
#knitrSet(lang='markdown', fig.path='png/', fig.align='left', w=6.5, h=4.5, cache=FALSE)
# If using blogdown: knitrSet(lang='blogdown')

#`r hidingTOC(buttonLabel="Outline")`

```
# Randomization {.tabset}

Randomized controlled trials (RCTs) play a special role in how scholars about
their theories and about how policy-makers learn about the success of their
programs because of randomization. Although the basic idea is simple, we have
made this module separate from others because randomization is central to the
enterprise, to help you understand just what it is and why it might play the
role that it does, and to provide some guidance about implementing it (and
avoiding common mistakes).

In this module we define randomization, separate random selection from random assignment, discuss common ways randomization is done and applied in designs.

## Core content

- What is **randomization**? (Random assignment is **not** the same as random sampling.)

- Four common ways to randomize treatment:
    - **Simple**: randomly assign units to treatment (like a coin flip)
    - **Complete**: within a list of eligible units, a assign a fixed number
      to receive a treatment (like drawing from a urn)
    - **Cluster**: assign groups or clusters of observations to the same
      treatment condition
    - **Block**: assign treatment within specific strata or blocks (as if
      you are running an experiment within each block)

- Some commonly used designs:
    - Randomized **access**: randomization to availability of a treatment
    - Randomized **delayed access**: randomize the timing of access
    - **Factorial**: randomize units to combinations of treatment arms
    - **Encouragement**: randomize the invitation to receive treatment

- How do you check whether your randomization produced balance on observables?  Typically we conduct
  randomization tests also known as balance tests using the $d^2$ omnibus test from `xBalance` in the `RItools` package (because it is randomization inference) or approximate this result with an $F$-test.

- There are, of course, **limits to randomization**.  We discuss some here and refer to the session on [Threats](threats-to-internal-validity-of-randomized-experiments.html) for more.

## Slides

Below are slides with the core content that we cover in our lecture on randomization. You can directly use these slides or make your own local copy and edit.

- [R Markdown Source](Resources/Slides/randomization-slides.Rmd)

- [PDF version](Resources/Slides/randomization-slides.pdf)

- [HTML version](Resources/Slides/randomization-slides.html)

The linked files shows how to [do replicable randomization in R](Resources/Exercises/randomization-exercises.Rmd). You can also see more examples of randomization in R at  [10 Things You Need to Know About Randomization](https://egap.org/resource/10-things-you-need-know-randomization).

You can also see the slides used in previous EGAP Learning Days:

- [The design issues presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019 (first section reviews randomization designs)](Resources/Slides/Examples/threats-benin.pdf)

- [The randomization presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018](Resources/Slides/Examples/randomization-montevideo.pdf)

- [The randomization presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019](Resources/Slides/Examples/randomization-bogota.pdf)

- [The randomization presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017](Resources/Slides/Examples/randomization-guatemala.pdf)

- [The randomization presentation from EGAP Learning Days in Salima, Malawi, February 2017](Resources/Slides/Examples/randomization-malawi.pdf)

- [The randomization presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016](Resources/Slides/Examples/randomization-santiago.pdf)


## Resources

### EGAP Methods Guides

- EGAP Methods Guide [10 Things You Need to Know About Randomization](https://egap.org/resource/10-things-you-need-know-randomization)

- EGAP Methods Guide [10 Things You Need to Know About Cluster  Randomization](http://egap.org/resource/10-things-you-need-know-about-cluster-randomization)

### Books, Chapters, and Articles

- [Standard operating procedures for Don Green’s lab at Columbia University](https://github.com/acoppock/Green-Lab-SOP). A comprhensive set of procedures and rules of thumb for conducting experimental studies.

- Running Randomized Evaluations: A Practical Guide, Chapter 4: Randomizing [@glennerster_running_2013].

- Field Experiments: Design, Analysis, and Interpretation, Chapter 2: Causal Inference and Experimentation [@gerber_field_2012].

### EGAP Policy Briefs

- Factorial Design
  - [How Media Influence Social Norms: Evidence from Mexico](https://egap.org/content/brief-57-how-media-influence-social-norms-evidence-mexico)
  - [Does Bottom-Up Accountability Work?](https://egap.org/resource/does-bottom-up-accountability-work-evidence-from-uganda/)
- Access
  - [Reducing Elite Capture in the Solomon Islands](https://egap.org/resource/brief-24-reducing-elite-capture-in-the-solomon-islands/)
- Delayed access
  - [Reducing Youth Support for Violence through Training and Cash Transfers in Afghanistan](https://egap.org/resource/reducing-youth-support-for-violence-through-training-and-cash-transfers-in-afghanistan/)
  - [Reducing Reconvictions Among Released Prisoners](https://egap.org/resource/brief-35-reducing-reconvictions-among-released-prisoners/)
- Cluster design
  - [Getting Out the Vote](https://egap.org/resource/brief-22-getting-out-the-vote/)
- Block and Cluster design
  - [Reporting Corruption](https://egap.org/resource/reporting-corruption-in-nigeria-testing-the-effects-of-norms-nudges/)
  - [Incumbent Malfeasance Revelations](https://egap.org/resource/evidence-from-mexico-the-effect-of-incumbent-malfeasance-revelations/)
- Encouragement design
  -  vote turn out
- Natural experiment (no randomization)
  - [Violent Conflict and Behavior in Burundi](https://egap.org/resource/brief-34-violent-conflict-and-behavior-in-burundi/)

<!--chapter:end:randomization.Rmd-->

```{r include=FALSE, cache=FALSE}
## Currently empty.

#require(Hmisc)    # provides knitrSet and other functions
#knitrSet(lang='markdown', fig.path='png/', fig.align='left', w=6.5, h=4.5, cache=FALSE)
# If using blogdown: knitrSet(lang='blogdown')

#`r hidingTOC(buttonLabel="Outline")`

```
# Hypothesis Testing {.tabset}


If we cannot directly observe how a person or a village might react to *both* a treatment *and* a control intervention because of the *fundamental problem of counterfactual causal inference* (as described in [the Causal Inference Module](causal-inference.html)), how can we learn about cause and effect using what we observe? In a randomized experiment, we can assess *guesses about the unobserved causal effects* by comparing what we observe in a given experiment to what we would observe if we were able to repeat the experimental manipulation *and the guess or claim or hypothesis* were true.

In this module we introduce hypothesis testing, how it relates to causal inference, what a hypothesis test is, and what to do when you have multiple hypothesis to test.


## Core content

- What is a **good hypothesis**?

- What is the relationship between hypothesis testing and causal inference? Why test hypotheses when we want to learn about the causal effect of some intervention on some outcome?

- What is a **hypothesis test**?

     - What is a null hypothesis?

     - Estimators versus test statistics.

     - In an experiment, a reference distribution for a hypothesis test comes from the experimental design and the randomization.

     - What is a $p$-value? How should we interpret the results of hypothesis tests?

- What do we want from a hypothesis test?

     - A good test casts doubt on the truth rarely (i.e., has a controlled and low false positive rate)

     - A good test easily distinguishes signal from noise (i.e., casts doubt on falsehoods often; has high statistical power)

- How would we know when our hypothesis test is doing a good job? ([Power analysis](poweranalysis-slides.Rmd) is its own module)

    - What is a false positive rate? What is correct coverage of a confidence interval? (And why are we mentioning confidence intervals when we talk about hypothesis tests?)

    - How might we assess the false positive rate of a hypothesis test for a given design and choice of test statistic? (The case of cluster-randomized trials and robust cluster standard errors.)

- Be careful when testing **many hypotheses** (for example, if you have more than two treatment arms or if you are assessing the effects of a treatment on multiple outcomes).  The following kinds of questions leads to multiple comparisons:

     - Does the effect of an experimental treatment differ between different groups? Could differences in treatment effect arise because of some background characteristics of experimental subjects?

     - Which, among several, strategies for communication were most effective on a single outcome?

     - Which, among several outcomes, were influenced by a single experimental intervention?

- For these questions, we have many tests/intervals, and we can easily mislead ourselves into thinking we have made a discovery when we have only rejected the null by chance.  We should be careful to **adjust the $p$-values or confidence intervals** to reflect the number of tests/intervals produced.

## Slides

Below are slides with the core content that we cover in our lecture on hypothesis testing. You can directly use these slides or make your own local copy and edit.

 - [R Markdown Source](https://egap.github.io/learningdays-resources/Slides/hypothesistesting-slides.Rmd)

 - [PDF Version](https://egap.github.io/learningdays-resources/Slides/hypothesistesting-slides.pdf)

 - [HTML Version](https://egap.github.io/learningdays-resources/Slides/hypothesistesting-slides.html)

You can also see the slides used in previous EGAP Learning Days:

 - [The hypothesis testing presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019](Resources/Slides/Examples/hypothesistesting-benin.pdf)

 - [The hypothesis testing presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018](Resources/Slides/Examples/hypothesistesting-montevideo.pdf)

 - [The hypothesis testing presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019](Resources/Slides/Examples/hypothesistesting-bogota.pdf)

 - [The hypothesis testing presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017](Resources/Slides/Examples/hypothesistesting-guatemala.pdf)

 - [The hypothesis testing presentation from EGAP Learning Days in Salima, Malawi, February 2017](Resources/Slides/Examples/hypothesistesting-malawi.pdf)

 - [The hypothesis testing presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016](Resources/Slides/Examples/hypothesistesting-santiago.pdf)



## Resources

### EGAP Methods Guides

  - [10 Things to Know About Hypothesis Testing](https://egap.org/resource/10-things-to-know-about-hypothesis-testing/)

  - [10 Things You Need to Know about Multiple Comparisons](https://egap.org/resource/10-things-to-know-about-multiple-comparisons/)

### Books, Chapters, and Articles

 - [Gerber, Alan S., and Donald P. Green. 2012. Field Experiments: Design, Analysis, and Interpretation. New York, NY: W. W. Norton & Company.](@gerber_field_2012), Chapter 3: Sampling Distributions, Statistical Inference, and Hypothesis Testing
 - [Rosenbaum, P R. 2010. “Design of observational studies.” Springer Series in Statistics. New York [etc.]: Springer.](@rosenbaum2010design), Chapter 2: Causal Inference in Randomized Experiments

<!--  ### EGAP Policy Briefs -->

<!-- ### Tools

 - [Exercises on hypothesis testing](Resources/Exercises/exercise-hypothesistesting.Rmd)

 - [Exercise on hypothesis testing using R](Resources/Exercises/r-exercise-hypothesistesting.Rmd)

-->

<!--chapter:end:hypothesistesting.Rmd-->

```{r include=FALSE, cache=FALSE}
## Currently empty.

#require(Hmisc)    # provides knitrSet and other functions
#knitrSet(lang='markdown', fig.path='png/', fig.align='left', w=6.5, h=4.5, cache=FALSE)
# If using blogdown: knitrSet(lang='blogdown')

#`r hidingTOC(buttonLabel="Outline")`

```
# Estimands and Estimators {.tabset}

In a randomized experiment, we can make guesses about the *average outcome under treatment* and the *average outcome under control* that are not systematically different from the truth. That is, we can write down unbiased estimators of the unobserved average treatment effect. Since averages are of widespread policy and scientific interest, the fact that randomized experiments allow us to make good guesses about them has encouraged the use of estimators of point effects, and the use of measures of how these estimators might vary from experiment to experiment in the form of standard errors and confidence intervals.

In this module we introduce types of estimands for our hypothesis and how we can analyse experimental data using an estimator to estimate our estimand and noise (standard error, confidence intervals), and the role of covatiate adjustments.

## Core content

- A **causal effect**, $\tau_i$, is a comparison of unobserved potential outcomes for each unit $i$.  For example, this can be a difference or a ratio of unobserved potential outcomes.  <!--: examples  $\tau_{i} = Y_{i}(Z_{i}=1) -  Y_{i}(Z_{i}=0)$ or  $\tau_{i} = \frac{Y_{i}(Z_{i}=1)}{ Y_{i}(Z_{i}=0)}$.-->

- To learn about $\tau_{i}$, we can treat $\tau_{i}$ as an **estimand** or target quantity to be estimated (this module) or as a target quantity to be hypothesized about ([Hypothesis Testing Module](hypothesis-testing.html)).

- Many focus on the **Average Treatment Effect (ATE)**, $\bar{\tau}=\sum_{i=1}^n
   \tau_{i}$, in part, because it allows for easy **estimation**.

- An **estimator** is a recipe for calculating a guess about the value of an estimand. For example, the difference of observed means for $m$ treated units is one estimator of $\bar{\tau}$. <!--:
   $\hat{\bar{\tau}} = \sum_{i=1}^n (Z_i Y_i)/m - \sum_{i=1}^n ( ( 1 - Z_i)
   Y_i)/(n-m)$.-->

- Different randomizations will produce different values of the same estimator targeting the same estimand. A **standard error** summarizes this variability in an estimator.

- A $100(1-\alpha)$% **confidence interval** is a collection of hypotheses that cannot be rejected at the $\alpha$ level. We tend to report confidence intervals containing hypotheses about values of our estimand and use our estimator as a test statistic.

- **Estimators should** (1) avoid systematic error in their guessing of the estimand (be unbiased); (2) vary little in their guesses from experiment to experiment (be precise or efficient); and perhaps ideally (3) converge to the estimand as they use more  and more information (be consistent).

 - **Analyze as you randomize** in the context of estimation means that (1) our standard errors should measure variability from randomization and (2) our estimators should target estimands defined in terms of potential outcomes.

 - We do not **control for** background covariates when we analyze data from randomized experiments. But covariates can make our estimation more **precise**. This is called **covariance adjustment**. **Covariance adjustment** in randomized experiments differs from controlling for in observational studies.

 - One can create a design that aims to assess whether causal effects vary by subgroup using blocking or by pre-registering subgroup based analyses (for example, comparing the average effect among group A with the average effect among group B).

 - A policy intervention (like a letter encouraging exercise) may *intend* to change behavior via an *active dose* (actual exercise). We can learn about the causal effect of the intention by randomly assigning letters (this is the **Intent to Treat Effect**). We can learn about the causal effect of actual execise by using the random assignment of letters as an **instrument** for the **Active dose** (exercise itself) in order to learn about the causal effect of exercise **among those who would change their behavior after reading the letter** (the average causal effect versions of these effects are often known as the **Complier Average Causal Effect** or the **Local Average Treatment Effect**).



## Slides

Below are slides with the core content that we cover in this session.  At the moment, we do not cover block randomization, cluster randomization, binary outcomes, or covariate adjustment.  Please refer to Chapter 3 in @gerber_field_2012 and see the links and citations in the [estimatr package for R](https://declaredesign.org/r/estimatr/).

- [R Markdown Source](Resources/Slides/estimation-slides.Rmd)

- [PDF Version](Resources/Slides/estimation-slides.pdf)

- [HTML Version](Resources/Slides/estimation-slides.html)

You can also see the slides used in previous EGAP Learning Days:

 - [The estimation presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019](Resources/Slides/Examples/estimation-benin.pdf)

 - [The estimation presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018](Resources/Slides/Examples/estimation-montevideo.pdf)

 - [The estimation presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019](Resources/Slides/Examples/estimation-bogota.pdf)

 - [The estimation presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016](Resources/Slides/Examples/estimation-santiago.pdf)

You can also discussion of the problems of estimating the effect of the active
dose of a treatment in these slides (as well as discussion of the problems that
missing data on outcomes cause for estimation of average causal effects):

- [The design issues presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019 (first section reviews randomization designs)](Resources/Slides/Examples/threats-benin.pdf)

- [The spillovers and attrition presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017](Resources/Slides/Examples/spillovers_attrition-guatemala.pdf)

- [The threats presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017](Resources/Slides/Examples/threats-guatemala.pdf)

- [The complications presentation from EGAP Learning Days in Salima, Malawi, February 2017](Resources/Slides/Examples/complications-malawi.pdf)

- [The threats presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016 (the middle section reviews ITT and non-compliance )](Resources/Slides/Examples/threats-santiago.pdf)


## Resources

### EGAP Methods Guides

 - EGAP Methods Guide [10 types treatment effect you should know about](https://egap.org/resource/10-types-treatment-effect-you-should-know-about)

 - EGAP Methods Guide [10 things to know about covariate adjustment](https://egap.org/resource/10-things-to-know-about-covariate-adjustment/)

 -  EGAP Methods Guide [10 Things to Know about Missing Data](https://egap.org/resource/10-things-to-know-about-missing-data/)

 -  EGAP Methods Guide[10 Things to Know about the Local Average Treatment Effect (effects on Compliers and Non-compliance)](https://egap.org/resource/10-things-to-know-about-the-local-average-treatment-effect/)

 -  EGAP Methods Guide [10 Types of Treatment Effect You Should Know About](https://egap.org/resource/10-types-of-treatment-effect-you-should-know-about/)

 -  EGAP Methods Guide [10 Things to Know about Spillovers](https://egap.org/resource/10-things-to-know-about-spillovers/)

### Books, Chapters, and Articles

 - @gerber_field_2012, Chapter 2.7 (on Excludability and Non-interference), Chapter 3, Chapter 5 on One-sided noncompliance, Chapter 6 on Two-Sided Noncompliance, Chapter 7 on  Attrition, Chapter 8 on Interference between Experimental Units

 - @bowers2020causality


### Tools

- [DeclareDesign](https://declaredesign.org)

- [estimatr package for R](https://declaredesign.org/r/estimatr/)


<!--chapter:end:estimation.Rmd-->

```{r include=FALSE, cache=FALSE}
## Currently empty.

#require(Hmisc)    # provides knitrSet and other functions
#knitrSet(lang='markdown', fig.path='png/', fig.align='left', w=6.5, h=4.5, cache=FALSE)
# If using blogdown: knitrSet(lang='blogdown')

#`r hidingTOC(buttonLabel="Outline")`

```
# Statistical Power and Design Diagnosands {.tabset}

Before we run a study, we would like to know whether a particular design has the statistical power to detect an effect if it exists.  It is difficult to learn from an under-powered study and a power analysis can help you improve your design or even decide against conducting the study.

In this module, we introduce statistical power, core approaches to calculating power analytically or through simulation, and show how design features such as blocking, covatiate adjustment and clustering impact on power.

## Core Content

 - **Statistical power** is the ability of a study to detect an effect given that it exists.
 
 - **Power analysis** is something we do before a study as it helps you figure out the sample you need, or what effects you can detect. It is an essential step in research design and helps you communicate about your design.

 - Common approaches to power calculation:
      - **Analytical** power calculations (using a formula)
      - Using **simulations** (for example using DeclareDesign)

 - **Covariate adjustment** and **blocking** can increase power.

 - For **clustered designs** you need to take account of the intra-cluster correlation (the within cluster variance relative to the overall variance).
 
 - Power is closely liked to your [study design](causal-inference.html), [hypothesis testing](hypothesis-testing.html) and [estimation](estimands-and-estimators.html)
 
 - See also the [glossary of terms](glossary-of-terms.html).

## Slides
Below are slides with the core content that we cover in our lecture on power. You can directly use these slides or make your local copy and edit.

 - [R Markdown Source](Resources/Slides/power-slides.Rmd)

 - [PDF version](Resources/Slides/power-slides.pdf)

 - [HTML version](Resources/Slides/power-slides.html)

You can also see the slides used in previous EGAP Learning Days:

 - [The power presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019](Resources/Slides/Examples/power-benin.pdf)

 - [The power presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019](Resources/Slides/Examples/power-bogota.pdf)

 - [The power presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018](Resources/Slides/Examples/power-montevideo.pdf)

 - [The power presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017](Resources/Slides/Examples/power-guatemala.html)

 - [The power presentation from EGAP Learning Days in Salima, Malawi, February 2017](Resources/Slides/Examples/power-malawi.pdf)

 - [The power presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016](Resources/Slides/Examples/power-santiago.pdf)


## Resources

### EGAP Methods Guide

 - EGAP Methods Guide[10 Things to Know About Statistical Power](https://egap.org/resource/10-things-you-need-know-about-statistical-power/)

 - EGAP Methods Guide [10 Things to Know about Covariate Adjustment](https://egap.org/resource/10-things-to-know-about-covariate-adjustment/)
 
 - EGAP Methods Guide [10 Things Your Null Results Might Mean](https://egap.org/resource/10-things-your-null-result-might-mean/)

### EGAP Policy Briefs and PAPs

Some examples of power analysis in designs:

 - [Pre-Analysis Plan. Accountability Can Transform (ACT) Health: A Replication and Extension of Bjorkman and Svensson (2009)](https://osf.io/qxwmu/)
 
 - [EGAP Policy Brief 58: Can bottom-up accountability generate improvements in health outcomes?](https://egap.org/resource/does-bottom-up-accountability-work-evidence-from-uganda/)


### Tools
 - Interactive power analysis
     - [EGAP Power Calculator](https://egap.shinyapps.io/power-app/)
     - [rpsychologist](https://rpsychologist.com/d3/NHST/)

 - R-Packages for power analysis
     - [pwr](https://cran.r-project.org/web/packages/pwr/index.html)
     - [DeclareDesign](https://cran.r-project.org/web/packages/DeclareDesign/index.html), see also <https://declaredesign.org/>


<!--chapter:end:statisticalpower.Rmd-->

```{r include=FALSE, cache=FALSE}
## Currently empty.

#require(Hmisc)    # provides knitrSet and other functions
#knitrSet(lang='markdown', fig.path='png/', fig.align='left', w=6.5, h=4.5, cache=FALSE)
# If using blogdown: knitrSet(lang='blogdown')

#`r hidingTOC(buttonLabel="Outline")`

```
# Measurement {.tabset}

To measure an outcome of interest and test hypothesis, we often use quantitative data derived from surveys, games or administrative records. For causal questions we typically use data on immediate and final outcomes and core mechanisms. We use baseline data to identify relevant subgroups, adjust our estimates or help block randomize our treatment. Data can be noisy (random error) and biased (systematic error). Measurement should be valid and reliable. 

In this module we discuss *what to measure* and *how to measure* and show how good measurement is closely linked to your [research design](Resources/Exercises/design-form.Rmd) and  [statistical power](statistical-power-and-design-diagnosands.html). 

## Core content

 - When we represent some attribute of unit by some number, letter, word, symbol in some systematic way (perhaps in a cell in a simple dataset), we are **measuring**.

 - A **valid** measure of a concept or phenomenon of interest should clearly
   represent that underlying and often abstract entity. For example, does
   question X measure "math ability"? It does so well, at first glance, if a
   community of scholars all agrees that people with higher values of question
   X or test Y do indeed have higher math ability. 

 - A **reliable** measure of a concept would provide the same score for the
   unit of measurement (a person, a village) if conditions were not changed.
   For example, an unreliable measure of the concept of "length" might be a
   rubber meter stick used by a 5 year old (who sometimes thinks of it as a
   sword).

 - We can assess our theories of measurement using: manipulation checks of the
   treatment variables; and also multiple, different approaches to measuring
   outcomes; covariates; or differences between units implied by different
   accounts of causal mechanisms. (Sometimes measurement is so hard that a
   pilot study focusing on measurement is called for.)

 - Invalid measurement can make it hard for your [research design](Resources/Exercises/design-form.Rmd) to effectively counter
   alternative explanations for the relationship between treatment and outcome.

 - Unreliable measurement can diminish [statistical power](statistical-power-and-design-diagnosands.html).

## Slides

Below are slides with the core content that we cover in our lecture on measurement. You can directly use these slides or make your own local copy and edit.

 - [R Markdown Source](Resources/Slides/measurement-slides.Rmd)

 - [PDF Version](Resources/Slides/measurement-slides.pdf)

 - [HTML Version](Resources/Slides/measurement-slides.html)

## Resources

### EGAP Methods Guides

- EGAP Methods Guide [10 Things to Know about Measurement in Experiments](https://egap.org/resource/10-things-to-know-about-measurement-in-experiments/)

- EGAP Methods Guide [10 Things to Know About Survey Design](https://egap.org/resource/10-things-to-know-about-survey-design/)

- EGAP Methods Guide [10 Things to Know About Survey Implementation](https://egap.org/resource/10-things-to-know-about-survey-implementation/)

### Books, Chapters, and Articles

 - [Adcock, Robert, and David Collier. 2001. “Measurement Validity: A Shared Standard for Qualitative and Measurement Validity: A Shared Standard for Qualitative and Quantitative Research.” American Political Science Review 95 (3): 529–46.](@adcocoll:2001)

 - [Scacco, Alexandra, and Shana S. Warren. 2018. “Can Social Contact Reduce Prejudice and Discrimination? Evidence from a Field Experiment in Nigeria.” American Political Science Review 112 (3): 654–77](@scacco_can_2018)
 
 - [Shadish, William R, Thomas D Cook, Donald Thomas Campbell, and others. 2002. Experimental and Quasi-Experimental Designs for Generalized Causal Inference/William R. Shedish, Thomas d. Cook, Donald T. Campbell. Boston: Houghton Mifflin](@shadish2002experimental)

 - [Vicente, Pedro C. 2014. “Is Vote Buying Effective? Evidence from a Field Experiment in West Africa.” Economic Journal 124 (574): F356–87](@vicente_is_2014)

### EGAP Policy Briefs

Examples of measurement strategies:

- Survey data at multiple levels
    - [EGAP Policy Brief 58: Does Bottom-Up Accountability Work?](https://egap.org/resource/does-bottom-up-accountability-work-evidence-from-uganda/)  
- Text messages
    - [EGAP Policy Brief 27: ICT and Politicians in Uganda ](https://egap.org/resource/brief-27-ict-and-politicians-in-uganda/)  
    - [EGAP Policy Brief 56: Reporting Corruption in Nigeria](https://egap.org/resource/reporting-corruption-in-nigeria-testing-the-effects-of-norms-nudges/)  
- Administrative data
    - [EGAP Policy Brief 67: Electoral Administration in Kenya](https://egap.org/resource/electoral-administration-in-kenya/)   
    - [EGAP Policy Brief 16: Spillover Effects of Observers in Ghana](https://egap.org/resource/brief-16-spillover-effects-of-observers-in-ghana/)  

<!--chapter:end:measurement.Rmd-->

```{r include=FALSE, cache=FALSE}
## Currently empty.

#require(Hmisc)    # provides knitrSet and other functions
#knitrSet(lang='markdown', fig.path='png/', fig.align='left', w=6.5, h=4.5, cache=FALSE)
# If using blogdown: knitrSet(lang='blogdown')

#`r hidingTOC(buttonLabel="Outline")`

```
# Threats to internal validity of randomized experiments {.tabset}

Randomized experiments can run into issues that can threaten the internal validity of randomized experiments. Some units might be missing outcome data and that missingness may be due to the treatment.  They may not take the treatment status assigned to them or be subject to spillover effects from a treated neighbor. 

In this module, we covers some common threats and some best practices to avoid or work around them.

## Core content {.tabset}

 - Review the three core assumptions discussed in the [Causal Inference Module](causal-inference.html).

 - We have said "Analyze as you randomize" elsewhere ([Estimands and Estimators Module](estimands-and-estimators.html)). Remember that you randomized treatment **assignment**, not receiving treatment or participating in data collection.  

 - **Missing data on the outcome (attrition)** is especially a problem if the patterns of missingness are caused by the treatment itself. This is the most common problem.

     - Do not drop observations that are missing outcome data from your analysis.

     - You may be able to **bound** treatment effect estimates.


 
 - **Non-compliance**. The effect of treatment assignment is not the same as the effect of receiving the treatment.  Sometimes units will not comply with their assigned treatment status, but not all hope is lost.
 
     - One-sided compliance when some units assigned to treatment fail to take the treatment, but all units assigned to control do not take the treatment.
 
     - The "local average treatment effect" (LATE, also known as the "complier average causal effect," CACE) is the average effect for the units who take the treatment when assigned, but not otherwise. If the monotonicity assumption and the exclusion restriction hold, we may be able to estimate LATE.



 - **"Spillover effects" or interference between units** is a violation of one of the core assumptions for causal inference ([Causal Inference](causalinference.html)).
     
      - However, this may not be a problem if you are interested in spillover effects and/or have designed your research to account for it.
 
     
 
 - **Hawthorne effects** are when subjects behave differently because they are being observed.  
 
- **Non-excludability** Treating treatment and control units differently, such as with different data collection processes or extra attention to the treated units, can be confuse interpretation of experimental results.

     - If Hawthorne effects are present for treated units but not control units, then we have a violation of the excludability assumption.  
     


 <!-- -- Endogenous subgroups, Conditioning on Post-Treatment Variables (improper Mediation Analysis) -->
 <!-- - The effect of **receipt** of treatment is often called the "Local Average Treatment Effect" or "Complier Average Causal Effect." "Local" refers to the idea that the effect only occurs on the people who take a dose (the kinds of people).   --> 


## Slides

Below are slides with the core content that we cover in our lecture on threats. You can directly use these slides or make your local copy and edit.

 - [R Markdown Source](Resources/Slides/threats-slides.Rmd)

 - [PDF version](Resources/Slides/threats-slides.pdf)

 - [HTML version](Resources/Slides/threats-slides.html)

You can also see the slides used in previous EGAP Learning Days:

- [The attrition and missing data presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019](Resources/Slides/Examples/)

- [The threats presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019 (first section reviews randomization designs)](Resources/Slides/Examples/threats-benin.pdf)

## Resources

### EGAP Methods Guide

- EGAP Methods Guide [10 Things to Know About Missing Data](https://egap.org/resource/10-things-to-know-about-missing-data/)

- EGAP Methods Guide [10 Types of Treatment Effect You Should Know About](https://egap.org/resource/10-types-of-treatment-effect-you-should-know-about/)

- EGAP Methods Guide [10 Things to Know About The Local Average Treatment Effect](https://egap.org/resource/10-things-to-know-about-the-local-average-treatment-effect/)

### Books, Chapters, and Articles

- [Standard operating procedures for Don Green’s lab at Columbia University](https://github.com/acoppock/Green-Lab-SOP). A comprhensive set of procedures and rules of thumb for conducting experimental studies.

### EGAP Policy Briefs

- [EGAP Policy Brief 16: Spillover Effects of Observers in Ghana ](https://egap.org/resource/brief-16-spillover-effects-of-observers-in-ghana/)

- [EGAP Policy Brief 11: Election Observers and Fraud in Ghana ](https://egap.org/resource/brief-11-election-observers-and-fraud-in-ghana/)

<!--chapter:end:threats.Rmd-->

```{r include=FALSE, cache=FALSE}
## Currently empty.

#require(Hmisc)    # provides knitrSet and other functions
#knitrSet(lang='markdown', fig.path='png/', fig.align='left', w=6.5, h=4.5, cache=FALSE)
# If using blogdown: knitrSet(lang='blogdown')

#`r hidingTOC(buttonLabel="Outline")`

```
# Ethical Considerations {.tabset}

Experiments involve intervention. A research team (composed of well-meaning
members from universities, governments, and other organizations) has reason to
believe that a new invention will improve the lives of people and also hope to
learn about the causal effect of the intervention clearly enough to inform the
theories of change that justify this and other interventions in this place and
elsewhere. 

In an experiment, interventions are randomly assign and we compare the lives of people (their behavior, their attitudes, etc.) between the
status quo and the new intervention. *Notice that such an experiment involves
one group of humans changing the lives of another group humans.* When one
person influences the life of another the influencer has responsibilities not to harm the person being influenced. Folks working in government do this as a matter of course --- their very job is to provide food, shelter, safety, justice, etc.. to their publics. Randomized experiments help governmental actors learn more quickly about what works and what does not. Academics also have direct effects on their students, but their effects on the public usually is indirect. When academics and policy makers collaborate the effects on the public can be immediate. So, even if academics usually spend little time worrying about negative effects on the public, a randomized experiment is one moment where they should spend more time on this worry. 

This module discusses the core ethical topics, such as privacy and autonomy, and the basic principles relating to respect for persons, beneficence and justice, and how informed consent, helps communicate about these principles to study participants. This module reminds research teams to carefully assess how the intervention might change the lives of those exposed to it (as well as those not exposed) so that everyone proceeds with the experiment feeling sure that it will do no harm.


## Core content

 - Research must weigh the **potential benefits** from the knowledge to be gained
   from the research against the **potential harms** to human subjects
 - How would you feel if you were a research subject in your study? In the control group? In the treatment group? A relatively high status member of the community? A relatively low status member of the community?
 - Key tenets: **privacy** and **autonomy**
 - Basic Principles in the Belmont Report: **respect for persons, beneficence, justice**
 - **Informed consent**: Can you ensure that research subjects have the freedom to refuse to participate and/or drop out of the study if they feel they should?
 - Challenges for social science experimental research in general:
    - many more people may benefit (or suffer from) your intervention than directly participate in your study.
    - changing elections or corruption can produce large societal changes. Is this beyond the remit of research?

## Slides

Below are slides with the core content that we cover in this session.

 - [R Markdown Source](Resources/Slides/ethics-slides.Rmd)

 - [PDF Version](Resources/Slides/ethics-slides.pdf)

 - [HTML Version](Resources/Slides/ethics-slides.html)

## Resources

- [EGAP Research Principles and Work on Ethics](https://egap.org/ethics/)

- [Belmont Report](https://www.hhs.gov/ohrp/regulations-and-policy/belmont-report/index.html)

- [Institutional Review Boards in the US](https://www.youtube.com/watch?v=U8fme1boEbE)

- [Example: Research Ethics at Oxford University in the UK](https://researchsupport.admin.ox.ac.uk/governance/ethics)

- [Example: Ethics for Researchers in the EU](https://ec.europa.eu/research/participants/data/ref/fp7/89888/ethics-for-researchers_en.pdf)

- [Example: Research Ethics at the Universidad Catolica de Chile](http://eticayseguridad.uc.cl/)

### Books, Chapters, and Articles

- [Asiedu, Edward, Dean Karlan, Monica P Lambon-Quayefio, and Christopher R Udry. 2021. “A Call for Structured Ethics Appendices in Social Science Papers.” Working Paper 28393. Working Paper Series. National Bureau of Economic Research. doi:10.3386/w28393.](@Asieduetal2021ethics)

- [Evans, David K. 2021. “Towards Improved and More Transparent Ethics in Randomised Controlled Trials in Development Social Science.” Working Paper 565. Center for Global Development. https://www.cgdev.org/sites/default/files/WP565-Evans-Ethical-issues-and-RCTs.pdf.(@Evans2021ethics)

### EGAP Policy Briefs and PAPs

Examples of PAPs and Papers that discuss ethical issues:

- [Pre Analysis Plan: The Effects of Non-Food Item Vouchers in a Humanitarian Context The Case of the Rapid Response to Movements of Population Program in Congo](https://osf.io/eutx7/)

 - [Paper: Appendix E.1 in Countering violence against women by encouraging disclosure: A mass media experiment in rural Uganda](http://jasper-cooper.com/papers/Green_et_al.pdf)

<!--chapter:end:ethics.Rmd-->

```{r include=FALSE, cache=FALSE}
## Currently empty.

#require(Hmisc)    # provides knitrSet and other functions
#knitrSet(lang='markdown', fig.path='png/', fig.align='left', w=6.5, h=4.5, cache=FALSE)
# If using blogdown: knitrSet(lang='blogdown')

#`r hidingTOC(buttonLabel="Outline")`

```
# Glossary of Terms {.tabset}

Below are some core terms frequently used throughout the book and more broadly in discussions of randomized field experiments.

## Key Concepts

See the module on [causal inference](causalinference.html), [estimands and estimators](estimation.html).

- **Potential outcome $Y_i(Z)$** What outcome $Y$ that unit $i$ *would*
  have under treatment condition $Z$.  We think of these as fixed quantities.
  Z can be 0 (for control) or 1 (for treatment). See the module on [causal inference](causal-inference.html).
- **Treatment effect $\tau_i$ for unit $i$** The difference between potential
  outcomes under treatment and control, $Y_i(1)-Y_i(0)$. See the module on [causal inference](causal-inference.html).
- **Fundamental Problem of Counterfactual Causal Inference** We can't observe $Y_i(1)$ and $Y_i(0)$ for the same
  unit at the same time, so we can't get $\tau_i$ directly. See the module on [causal inference](causal-inference.html).
- **Estimand** The thing you want to estimate.  Example: average treatment
  effect. In counter-factual causal inference, this is a function of potential
  outcomes, not fully observed outcomes. See the module on [estimands and estimators](estimands-and-estimators.html).
- **Estimator** How you make a guess about the value of your estimand from the
  data you have.  Example: difference-in-means. See the module on [estimands and estimators](estimands-and-estimators.html).
  - **Average treatment effect, ATE** The average of the treatment effect for all
  individuals in your subject pool.  This is an **estimand**. $\overline{Y_i(1)-Y_i(0)}$, which is also
  equivalent to $\bar{Y}_i(1)-\bar{Y}_i(0)$. Notice that we do not use the
  $E[Y_i (1)]$ style of notation here because $E[]$ means "average over
  repeated operations" but $\bar{Y}$ means "average over a set of
  observations". See the module on [causal inference](causal-inference.html) and the module on [estimands and estimators](estimands-and-estimators.html).
- **Random Sampling** Selecting subjects from a population with known
  probabilities (strictly between 0 and 1).
- **$k$-Arm Experiment** An experiment that has $k$ treatment conditions
  (including control). See the module on [randomization](randomization.html).
- **Random Assignment** Assigning subjects with known probability (without
  replacement) strictly between 0 and 1 to experimental conditions.  This is
  the same as random sampling from the potential outcomes.  There are several
  strategies for random assignment: simple, complete, cluster, block, blocked
  cluster. See the module on [randomization](randomization.html).
- **External Validity** Findings from your study teach you about contexts
  outside of your sample --- other locations or in other interventions.

## Statistical Inference

See module on [hypothesis testing](hypothesistesting.html) and [statistical power](statisticalpower.html).

- **Hypothesis** Simple, clear, falsifiable claim about the world. In
  counter-factual causal inference, this is a statement about a relationship
  among potential outcomes, like $H_0: Y_i(Z_i=0) = Y_i(Z_i=1) + \tau_i$ for
  the hypothesis that the potential outcome to treatment is produced from the
  potential outcome to control plus some effect for each unit $i$. See the
  module on [hypothesis testing](hypothesis-testing.html).
- **Null Hypothesis**  A conjecture about the world that you may reject after
  seeing the data.  See the module on [hypothesis
  testing](hypothesis-testing.html).
- **Sharp Null Hypothesis of No Effect** The null hypothesis that there is no
  treatment effect for any subject.  This means $Y_i(1)=Y_i(0)$ for all $i$. We
  might write this,  $H_0: Y_i(Z_i=0) = Y_i(Z_i=1)$.  See the module on
  [hypothesis testing](hypothesis-testing.html).
- **$p$-value** The probability seeing a test statistic as large (in absolute
  value) as the observed test statistic (e.g., the difference in means) or
  larger.  See the module on [hypothesis testing](hypothesis-testing.html).
- **One-sided vs.~Two-sided Test** When you have a strong expectation that the
  effect is either positive or negative, you can conduct a one-sided test.
  When you do not have such a strong expectation, conduct a two-sided test.  A
  one-sided test has more power than a two-sided test for the same experiment.
  See the module on [hypothesis testing](hypothesis-testing.html).
- **Standard Deviation** Square root of the mean-square deviation from average
  of a variable.  It is a measure of the dispersion or spread of a statistic.
  $SD_x=\sqrt{\frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^2}$
- **False Positive Rate/Type I Error of a Test** A well-operating hypothesis
  test rejects a hypothesis about a true causal effect no more than $\alpha$ %
  of the time. The false positive rate is the rate at which a test will cast
  doubt on a true hypothesis, the rate at which the test will encourage the
  analyst to say "statistically significant" when, in fact, there is no causal
  relationship. See the module on [hypothesis
  testing](hypothesis-testing.html).
  - **Sampling Distribution** The distribution of estimates (e.g., estimates of
    the ATE) for all possible treatment assignments. In design-based
    statistical inference for randomized experiments, the distribution of
    estimates from an estimator is generated from randomizations. Many call
    this a "sampling distribution" because textbooks often use the idea of
    repeated samples from a population rather than repeated randomizations to
    describe this kind of variation.
  - **Standard Error** The standard deviation of the sampling distribution.  A
    bigger standard error means that our estimates are more susceptible to
    sampling variation. See the module on [estimands and
    estimators](estimands-and-estimators.html).
- **Coverage of a confidence interval** A well-operating confidence interval
  contains the true causal effect $100 ( 1 - \alpha)$ % of the time. A
  confidence interval that has *incorrect coverage* when it excludes the true
  parameter less than $100 (1 - \alpha)$% of the time (for example, a 95%
  confidence interval is supposed to only exclude the true parameter less than
  5% of the time).
- **Statistical Power of a Test**  Probability that a test of causal effects
  will detect a statistically significant treatment effect if the effect
  exists. See the module on [statistical power](statistical-power-and-design-diagnosands.html). This depends on:
    -  The number of observations in each arm of the experiment
    -  Effect size (usually measured in standardized units)
    -  Noisiness of the outcome variable
    -  Significance level ($\alpha$, which is fixed by convention)
    -  Other factors including what proportion of your units are assigned to
    different treatment conditions.
- **Intra-Cluster Correlation** How correlated the potential outcomes of units
  are within clusters compared to across clusters.  Higher intra-cluster
  correlation hurts power.
- **Unbiased** An estimator is unbiased if you *expect* that it will return the
  right outcome. That means that if you were to run the experiment many times,
  the estimate might be too high or to low sometimes but it will be right on
  average. See the module on [estimands and estimators](estimands-and-estimators.html).
- **Bias** Bias is the difference between the average value of the estimator
  across its sampling distribution and the single, fixed value of the estimand. See the module on [estimands and estimators](estimands-and-estimators.html).
- **Consistency of an estimator** An estimator that produces answers that
  become ever nearer the true estimand value as the sample size increases is a
  *consistent estimator* of that estimand. A consistent estimator may or may
  not be unbiased. See the module on [estimands and estimators](estimands-and-estimators.html).
- **Precision/Efficiency of an estimator** The variation in or width of the
  sampling distribution of an estimator. See the module on [estimands and estimators](estimands-and-estimators.html).



## Randomization Strategies

See the module on [randomization](randomization.html).

- **Simple** An independent coin flip for each unit.  You are not guaranteed
  that your experiment will have a specific number of treated units.
- **Complete** Assign $m$ out of $N$ units to treatment, i.e., you know how
  many units will be treated in your experiment.  Each unit has a $m/N$
  probability of being treated.  The number of ways treatment can be assigned
  (number of permutations of treatment assignment) is $\frac{N!}{m!(N-m)!}$.
- **Block** First divide the sample into blocks, then complete randomization in
  each block separately.  A block is a set of units within which you conduct
  random assignment.
- **Cluster** Clusters of units are randomly assigned to treatment conditions.
  A cluster is a set of units that will always be assigned to the same
  treatment status.
- **Blocked Cluster**  First form blocks of clusters.  Then in each block,
  randomly assign the clusters to treatment conditions using complete
  randomization.

## Factorial Designs

See the module on [randomization](randomization.html).

- **Factorial Design** A design with more than one treatment, with each
  treatment assigned independently.  The simplest factorial design is a 2 by 2.
- **Conditional Marginal Effect**  The effect of one treatment, conditional on
  the other being held at a fixed value. For example:
  $Y_i(Z_1=1|Z_2=0)-Y_i(Z_1=0|Z_2=0)$ is the marginal effect of $Z_1$
  conditional on $Z_2=0$.
- **Average Marginal Effect**  Main effect of each treatment in a factorial
  design.  It is the average of the conditional marginal effects for all the
  conditions of the other treatment, weighted by the proportion of the sample
  that was assigned to each condition.
- **Interaction Effect** In a factorial design, we may also estimate
  interaction effects.
    - No interaction effect: one treatment does not amplify or undercut the
    effect of the other treatment.
    - Multiplicative interaction effect:  the effect of one treatment depends on
    whether a unit was assigned the other treatment.  This means one treatment
    *does* amplify or undercut the effect of the other.  The effect of two
    treatments together is *not* the sum of the effect of each treatment.

## Threats

See the module on [more elaborate questions](more-elaborate-questions.html) and the module on [threats](threats-to-internal-validity-of-randomized-experiments.html).

- **Hawthorne Effects** When a subject responds to being observed.
- **Spillovers** When a subject responds to another subject's treatment status.
  Example: my health depends on whether my neighbor is vaccinated, as well as
  whether I am vaccinated.
- **Attrition** When outcomes for some subjects are not measured.  Example:
  people migrate or people die.  This is especially problematic for inference
  when correlated with treatment status.
- **Compliance** A unit's treatment status matches its assigned treatment
  condition.  Example of non-compliance: a unit assigned to treatment doesn't
  take it. Example of compliance: a unit assigned to control does not take
  treatment.
- **Compliance Types**  There are four types of units in terms of compliance:
  - **Compliers** Units that would take treatment if assigned to treatment and
    would be untreated if assigned to control.
  - **Always-Takers** Units that would take treatment if assigned to treatment
    and if assigned to control.
  - **Never-Takers** Units that would be untreated if assigned to treatment and
    if assigned to control.
  - **Defiers** Units that would be untreated if assigned to treatment and
    would take treatment if assigned to control.
- **One-sided Non-Compliance** The experiment has only compliers and
  *either* always takers or never takers.  Usually, we think of
  one-sided non-compliance as having only never takers and compliers meaning
  that that local average treatment effect is the effect of treatment on the
  treated.
- **Two-sided Non-Compliance** The experiment may have all four latent groups.
- **Encouragement Design**  An experiment that randomizes $Z$ (treatment
  assignment), and we measure $D$ (whether the unit takes treatment) and $Y$
  (outcome).  We can estimate the ITT and the LATE (Local Average Treatment
  Effect, aka CACE---Complier Average Causal Effect).  It requires three
  assumptions.
  - **Monotonicity**  Assumption of either no defiers or no compliers.  Usually
    we assume no defiers which means that the effect of assignment on take up
    of treatment is either positive or zero but not negative.
  - **First Stage** Assumption that there is an effect of $Z$ on $D$.
  - **Exclusion Restriction** Assumption that $Z$ affects $Y$ only through $D$.
    This is usually the most problematic assumption.
- **Intention-to-Treat Effect (ITT)** The effect of $Z$ (treatment assignment)
  on $Y$.
- **Local Average Treatment Effect (LATE)**  The effect of $D$ (taking
  treatment) on $Y$ for compliers.  Also known as Complier Average Causal
  Effect (CACE). Under the exclusion restriction and monotonicity, the LATE is
  equal to ITT divided by the proportion of your sample who are Compliers.
- **Downstream Experiment** An encouragement design study that takes advantage
  of the randomization of $Z$ by a previous study.  The outcome from that
  previous study is the $D$ in the downstream experiment.

<!--chapter:end:glossary.Rmd-->

```{r include=FALSE, cache=FALSE}
## Currently empty.

#require(Hmisc)    # provides knitrSet and other functions
#knitrSet(lang='markdown', fig.path='png/', fig.align='left', w=6.5, h=4.5, cache=FALSE)
# If using blogdown: knitrSet(lang='blogdown')

#`r hidingTOC(buttonLabel="Outline")`

```
# Introduction to R and RStudio {.tabset}

Throughout the book we include R code for estimation, similation, and creating examples. We used RStudio to create the slides. To personalize them for your own purpose, we assume you will use R+markdown. Below, we include guide on setting up R and RStudio on your machine, as well as some basic commands often used.

## R and RStudio

R is an free software environment most commonly used for statistical analysis and computation. Because Learning Days participants arrive with different statistical backgrounds and preferred statistical software, we will use R to ensure that everyone is on the same page. We advocate the use of R more generally for its flexibility, wealth of applications, and comprehensive online support (mostly forums). 

RStudio is a free, open source integrated development environment for with an user interface that makes R much more user-friendly. R Markdown, a feature of RStudio, enables the easy output of code, results, and text in a .pdf, .html, or .doc format. 

This document provides a tutorial on downloading R and RStudio in addition to an introduction to the interface.

## Downloading R and RStudio

### Downloading R

R can be freely downloaded from CRAN at the link corresponding to your operating system:

- For **Windows**: [https://cran.r-project.org/bin/windows/base/](https://cran.r-project.org/bin/windows/base/)
- For **Mac OS X**: [https://cran.r-project.org/bin/macosx/](https://cran.r-project.org/bin/macosx/).   
    - Select `R-3.4.3.pkg` for OS X 10.11 and higher.
    - Select `R-3.3.3.pkg` for OS X 10.9-10.11.
    - Select `R-3.2.1-snowleopard.pkg` for OS X 10.6-10.8.

### Downloading RStudio

RStudio can be freely downloaded from the RStudio website, [https://www.rstudio.com/products/rstudio/download/](https://www.rstudio.com/products/rstudio/download/). In the table, click the green `Download` button at the bottom of the left column, "RStudio Desktop Open Source License" as depicted below in Figure 1. Once you select this button, the page will jump to a list of download options as depicted in Figure 2 (page 3).

- For **Windows**, select `Windows Vista/7/8/10`.
- For **Mac OS X**, select `Mac OS X 10.6+ (64-bit)`.



```{r rstudiopng, out.width='32.8%', fig.show='hold', fig.cap='Select **Download** in the "RStudio Desktop Open Source License" column.'}
knitr::include_graphics(here("Resources/Images","new_rstudio.png"))
```

```{r rstudiodownload, out.width='32.8%', fig.show='hold', fig.cap='Select the Windows Vista/7/8/1 link for Windows or the Mac OS X 10.6+ (64-bit) link for Mac.'}
knitr::include_graphics("Resources/Images/rstudio_download.png")
```

## RStudio Interface

When you open RStudio for the first time, there should be three panels visible, as depicted in Figure 3 below.

- Console (left panel)
- Accounting (upper right panel): includes Environment and History tabs
- Miscellaneous (lower right panel)

```{r rstudiointro, out.width='32.8%', fig.show='hold', fig.cap='When you open RStudio, there are three panels visible: the Console (left), Accounting (upper right), and Miscellaneous (lower right).'}
knitr::include_graphics("Resources/Images/rstudio_intro.png")
```

### Console

One can execute all operations in the console. For example if one entered `4 + 4` and hit the Enter/Return key, the console will return `[1] 8`. 

To make sure everyone is prepared to use R at Learning Days, we ask you to run one line of code in the Console to download several R packages. Packages are fragments of reproducible code that allow for more efficient analysis in R. To run these lines, copy the following code into the console and hit your  `Return`/`Enter` key. Note that you must be connected to the internet to download packages.

```{r, eval = F}
install.packages(c("ggplot2", "dplyr", "AER", "arm", "MASS", "sandwich", "lmtest", "randomizr", "DeclareDesign"))
```

If successfully downloaded, your console will resemble Figure 4, though the urls will be different as a function of your location. 
```{r console2, echo=FALSE, out.width='32.8%', fig.show='hold',fig.cap='An image of the console after executing the three lines of code listed above.'}
knitr::include_graphics("Resources/Images/console2a.png")
```

### Editor 

In order to write and save reproducible code, we will open a fourth panel, the Editor, by clicking on the icon with a white page with a plus sign on the upper-left corner of the RStudio interface and selecting `R Script`, as depicted in Figure 5.

```{r newscript, out.width='32.8%', fig.show='hold',fig.cap='Create a new R script and open the editor panel by selecting `R Script` from the dropdown menu.'}
knitr::include_graphics("Resources/Images/new_script.png")
```

Once the R script is opened, there should be four panels within the RStudio interface, now with the addition of the Editor panel. We can execute simple arithmetic by entering a formula in the editor and pressing `Control + Enter` (Windows) or `Command + Enter` (Mac). The formula and the "answer" will appear in the Console, as depicted in Figure 6 (page 6), with red boxes for emphasis.


```{r firstaddition, out.width='32.8%', fig.show='hold', fig.cap='An arithmetic expression is entered in the editor and evaluated in the console. The red boxes are added for emphasis.'}
knitr::include_graphics("Resources/Images/first_addition.png")
```

R can be used for any arithmetic operation, including but not limited to: addition (`+`), subtraction (`-`), scalar multiplication (`*`), division (`/`), or exponentiation (`^`). 

### Accounting 

Beyond basic functions, we can also store values, data, and functions in the global environment. To assign a value to a variable, use the `<-` operator. All stored values, functions, and data will appear in the environment tab in the Accounting panel. In Figure 7, we define the variable `t` to take the value $3 \times \frac{6}{14}$, and can see that it is stored under Values. We also load a dataset. Here, "ChickWeight" is a dataset built into R; most datasets will be loaded from the web or other files on your computer through an alternate method. We can see that ChickWeight contains 578 observations of 4 variables and is stored in the Environment. By clicking on the name ChickWeight a tab will enter with the dataset in your Editor window.

```{r savedata, echo=FALSE, out.width='32.8%', fig.show='hold', fig.cap='The value 3 * (6/14) is assigned to the variable t (red) and the dataset ChickWeight is added to the global environment (blue). The boxes are added for emphasis.'}
knitr::include_graphics("Resources/Images/save_data.png")
```

R provides many tools to analyze and view data that we will discuss in more depth at Learning Days. For now, we can learn some basic tools to examine the data. The function `head()` allows us to see the first six rows of the dataset. `summary()` summarizes each of the columns of the dataset and `dim()` provides the dimensions of the dataset in terms of the number of rows and then columns. 

```{r, warning = F, message = F}
head(ChickWeight) # First 6 observations in dataset
summary(ChickWeight) # Summary of all variables
dim(ChickWeight) # Dimensions of the dataset in the order rows, columns
```

Unlike other statistical software, R allows for the storage of multiple datasets, possibly of different dimensions, simultaneously. As such it is quite flexible for analysis using multiple methods.

### Miscellaneous 

R provides a suite of tools, ranging from built-in plot functions to packages to graph data, models, and estimates, etc. The final "miscellaneous" panel allows for viewing of graphs with ease in RStudio. Figure 8 depicts a plot in this panel. We will discuss how to plot data during Learning Days; for now, don't worry about the graphing the code in the Editor. 

```{r graph, out.width='32.8%', fig.show='hold', fig.cap='An example plot of the `ChickWeight` data made in R.'}
knitr::include_graphics("Resources/Images/graph.png")
```

## Learning to Use R

### Online Resources

There are many helpful online resources to help you start learning R. We recommend two sources:

- Code School, which runs entirely through your browser [https://www.codeschool.com/courses/try-r](https://www.codeschool.com/courses/try-r). 
- Coursera, via an online R Programming course organized by Johns Hopkins University: 
    i. Go to [coursera.org](coursera.org)
    ii. Create an account (this is free!)
    iii. Sign up for R Programming at Johns Hopkins University (instructor: Roger Peng) under the "Courses" tab
    iv. Read the materials and watch the videos from the first week. The videos from the first week are about 2.5 hours long total.  
    
### Basic Practice

Here we provide some fragments of code to familiarize you with some basic practices in R. We recommend that you practice by typing the code fragments into your Editor and then evaluating them.

#### Setting up an R Session

In general, we read other files such as data or functions into R and output results like graphs or tables into files not contained within an R session. To do this, we must give R an "address" at which it can locate such files. It may be most efficient to do this by setting a working directory, a file path at which relevant files are stored. We can identify the current working directory using `getwd()` and set a new one using `setwd()`. Note that the syntax of these filepaths varies by operating system.

```{r}
getwd()
```

```{r, eval = F}
setwd("~TaraLyn/Dropbox/EGAP Learning Days Admin/Workshop 2018_2 (Uruguay)/")   
```

You may need to install packages beyond those listed above to execute certain functions. To install packages we use `install.packages("")`, filling in the package name between the "" marks, as follows. You need only install packages once.

```{r,eval=FALSE}
install.packages("Hmisc")  
```

Once a package is downloaded, it can be loaded and accessed using `library()` where the package name is inserted between the parentheses (no "" marks).

```{r, eval= F}
library(Hmisc)
```

To clear R's memory, namely the stored data, functions, or values that appear in the accounting tab, use `rm(list = ls())`. It may be useful to set a random number seed to ensure that replication is possible, particularly when we work with simulation-based methods.

```{r}
rm(list = ls())                                   
set.seed(2018)  # Optional: Set a seed to make output replicable
```

#### R Basics

We now explore some of the basic commands. In order to assign a scalar (single element) to a variable, we use the `<-` command as discussed previously: 

```{r,echo=TRUE}
(a <- 5)     # "<-"  is the assignment command; it is used to define things. eg:
```

We may also want to assign a vector of elements to a variable. Here we use the same `<-` command, but focus on how to create the vector.

```{r,echo=TRUE}
(b <- 1:10)              # ":"  is used to define a string of integers

(v <- c(1, 3, 2, 4, pi))   # use c() to make a vector with anything in it
```

We can then refer to elements of a vector by denoting their position in a vector inside hard brackets `[]`.

```{r, echo=TRUE}
# Extract elements of a vector:
b[1]                   # Returns position 1
b[5:4]                 # Returns positions 5 and 4, in that order
b[-1]                  # Returns all but the first number  

# Returns all numbers indicated as "TRUE"
b[c(TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE)]  
                                                                          
# Assign new values to particular elements of a vector
b[5] <- 0
```

There are a set of built-in functions that can be applied to vectors like `b`.

```{r, echo=TRUE}
sum(b)      # Sum of all elements
mean(b)     # Mean of all elements
max(b)      # Maximum of all elements
min(b)      # Minimum of all elements
sd(b)       # Standard deviation of all elements
var(b)      # Variance of all elements
```

We can also apply arithmetic transformations to all elements of a vector:

```{r, echo=TRUE}
b^2               # Square the variable
b^.5              # Square root of the variable
log(b)            # Log of variable
exp(b)            # e to the b
```

Finally, we can evaluate logical statements (i.e. ``is condition X true?'') on all elements of a vector:

```{r, echo=TRUE}
b == 2                     # Is equal to
b < 5                      # Less than
b >= 5                     # Greater than or equal to 
b <= 5 | b / 4 == 2        # | means OR
b>2 & b<9                  # & means AND
is.na(b)                   # Indicates if data is missing
which(b<5)                 # Gives indices of values meeting logical requirement
```

The basic logic of these commands applies to data structures much more complex than scalars and vectors. Understanding of these basic features will help facilitate your understanding of more advanced topics during Learning Days.

<!--chapter:end:intro_to_r.Rmd-->

```{r include=FALSE, cache=FALSE}
## Currently empty.

#require(Hmisc)    # provides knitrSet and other functions
#knitrSet(lang='markdown', fig.path='png/', fig.align='left', w=6.5, h=4.5, cache=FALSE)
# If using blogdown: knitrSet(lang='blogdown')

#`r hidingTOC(buttonLabel="Outline")`

```
`r if (knitr:::is_html_output()) '

# References {-}

'`

<div id="refs"></div>

<!--chapter:end:references.Rmd-->

