[["index.html", "Designing and Understanding Field Experiments: Resources from the EGAP Learning Days Module 1 Introduction 1.1 How to use the book 1.2 We love to hear from you! 1.3 Acknowledgments", " Designing and Understanding Field Experiments: Resources from the EGAP Learning Days Jake Bowers1, Maarten Voors2, Nahomi Ichino3 February 17, 2021 Module 1 Introduction Over the past decade, Evidence in Governance and Politics (EGAP) has organized Learning Days workshops on experimental methods for principal investigators (PIs) in Africa and Latin America with the aim of building experimental social-science research capacity among researchers and practitioners. By sharing rigorous research methods with workshop participants, the Learning Days hopes to identify and build researcher networks around the world and to create strong, productive connections between EGAP members and those researchers. The Learnings Days workshops are a combination of design clinics, research presentations, guided work with statistical software, and topical lectures by a small group of instructors, largely professors and PhD students from the EGAP Network. The workshops focus on methods for the design and analysis of randomized experiments in the field rather than on randomized experiments in the lab or non-randomized studies. This book grew out of a desire to share the materials we developed for the Learning Days. The current version is written primarily for instructors and organizers of similar workshops and courses aimed at principal investigators (PIs) — i.e., professors, post-doctoral fellows, PhD students, and NGO/government agency evaluators — who implement experimental research projects on programs related to institutions, governance, and development. Much of the material will also be useful as a refresher for past participants of the Learning Days workshops. This book is a comprehensive overview of causal inference methods for researchers developing an experimental research design. It is organized in modules and covers topics such causal inference, randomization, hypothesis testing, estimands, estimators, statistical power, threats to inference, and the ethics of experimentation. The modules appear in the order in which the Learning Days instructors have found most useful to present the material. However, the modules are linked to one another and can be reordered to suit your needs as an instructor. In the appendix, we include some course preliminaries including a glossary of terms, an overview of basic statistics, and an introduction to R and RStudio. The book includes slides on the core content, the EGAP research design form, and references to research examples and previous Learning Days materials. The material included here builds significantly on and links to EGAP’s work on methodology, summarized in the EGAP Methods Guides. We have also expanded on the material that is usually covered during the Learning Days workshops: the slides and modules presented here contain too much information to be covered in a single week. We present more rather than less information here, however, as an aid to instructors tailoring a local course to a specific audience. 1.1 How to use the book In order to most benefit from the book, please have R and RStudio installed on your machine. In fact, the slides assume you will use R+markdown to personalize them for your own purposes. To get going with R, see Getting Started in R and Simple Statistics. You can copy this book or parts thereof (e.g., slides, etc.) either by using the Download button on the front page of http://github.com/egap/learningdays-guide OR by using github directly (by forking this repository). We are happy for anyone to use the materials as long as EGAP is attributed. See Creative Commons Attribution-ShareAlike 4.0 International License for the precise terms. 1.2 We love to hear from you! If you have any questions, feedback or have organized your own event, please get in touch! Simply post an issue on Github or contact us via email, admin@egap.org. 1.3 Acknowledgments The materials included in this book have been developed over the past years by various Learning Days instructors. These include (in alphabetical order) Jake Bowers, Jasper Cooper, Ana De la O, Lindsay Dolan, Natalia Garbiras Díaz, Macartan Humphreys, Nahomi Ichino, Salif Jaiteh, Gareth Nellis, Dan Nielson, Rafael Piñeiro, Fernando Rosenblatt, Tara Slough, Peter van der Windt and Maarten Voors. At EGAP tremendous support has been provided by Matt Lisiecki, Ingrid Lee, Goldie Negelev, Max Mendez-Back and others. Learning Days have been generously funded by the Hewlett Foundation and supported by institutions around the world such as the African School of Economics (Benin), Universidad Diego Portales (Chile), Universidad de los Andes (Colombia), Ghana Center for Democratic Development (Ghana), Mercy Corps (Guatemala), Invest in Knowledge (Malawi), NYU Abu Dhabi (UAE), Universidad Catolica del Uruguay (Uruguay). University of Illinois @ Urbana-Champaign↩︎ Wageningen University↩︎ Emory University↩︎ "],["the-research-design-processes.html", "Module 2 The Research Design Processes 2.1 Core Content 2.2 Slides 2.3 Design Form and Pre-Registration 2.4 Resources", " Module 2 The Research Design Processes This book aims to help you design and field randomized field experiments. And the modules that follow dive into the details of causal inference and statistics that will help you make wise decisions about a given experiment. However, an experiment that is well designed from a statistical sense may not answer the right question, or it may address that question poorly. And, even if the experiment does answer a good question well, policy-makers and scholars may not believe the results if the analysis of the data or fieldwork is difficult for them to understand. In this module, we introduce the EGAP Research Design Form that we have created as a kind of checklist to guide you through the stages in the research process. We also point toward the DeclareDesign software package which enables us to explore the consequences of different research design decisions. We will use this form for the rest of the course. Finally, we use this module as a space to talk about pre-analysis planning and registration. When plan our analyses and make these plans public, we improve our chances of persuading others with our results. See for one policy example the white paper on Preregistration as a Tool for Strengthening Federal Evaluation from the US Government’s Office of Evaluation Sciences (you can also see examples of their pre-analysis plans on all of their field experiment pages). We cannot provide an easy recipe for finding and articulating a good scientific or policy question, we hope here to provide some space for discussion of these topics so that they are not ignored. 2.1 Core Content What makes a good research question? A good research question advances science and/or is a question the answer to which will change a policy decision. Certain research designs are better able to address certain questions. We want to choose the design that best answers our key questions within our constraints. The questions we ask arise, often implicitely, from our values and from our understandings about how the world works. These theories structure our questions — make the questions relevant. And the experiments that we execute teach us about the theory — we hope that the evidence and data arising from these research designs improves our understanding. What are the core parts of a research design? Introduce core components of the EGAP Research Design Form. Introduce a research design software package, DeclareDesign. The move in social science towards the review of designs, rather than outcomes. Pre-registration: what is it? why should we do it? how should we do it? (EGAP Methods Guide 10 Things to Know about Pre-Analysis Plans ) 2.2 Slides Below are slides with the core content that we cover in our lecture on research desuign. You can directly use these slides or make your own local copy and edit. R Markdown Source PDF Version HTML Version You can also see the slides used in previous EGAP Learning Days: The DeclareDesign presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018 The DeclareDesign presentation from EGAP Learning Days in Salima, Malawi, February 2017 The DeclareDesign presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016 You can also see slides for Design Talks in previous EGAP Learning Days, where presenters focus on issues that come up in designing the research, rather than the results: Design Talk from EGAP Learning Days at African School of Economics, Benin, March 2018 Design Talk from EGAP Learning Days in Salima, Malawi, February 2017 Design Talk 1 from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018 Design Talk 2 from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018 Design Talk 3 from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018 Design Talk 1 from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016 Design Talk 2 from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016 Design Talk 3 from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016 Design Talk from EGAP Learning Days in Guatemala City, Guatemala, August 2017 2.3 Design Form and Pre-Registration Research Design Form Links to repositories for pre-registration/pre-analysis plans: EGAP registry, hosted by OSF (https://egap.org/registry/) AEA RCT registry (https://www.socialscienceregistry.org/) OSF (https://osf.io/registries) Examples of other pre-registrations/pre-analysis plans: SMS Messages in Mozambique from the US Federal Government Police Body-Cameras from the Lab @ DC 2.4 Resources 2.4.1 EGAP Methods Guides EGAP Methods Guide 10 Things to Know about Pre-Analysis Plans EGAP Methods Guide 10 Things to Know about Measurement in Experiments 2.4.2 Books, Chapters, and Articles Christensen, Garret S., Jeremy Freese, and Edward Miguel. 2019. Transparent and Reproducible Social Science Research: How to Do Open Science. Oakland, California: University of California Press.. A great book that summarizes new approaches in social science research on transparency and reproducibility. 2.4.3 Tools DeclareDesign, an exciting and comprehensive set of software tools for describing, assessing, and conducting empirical research. "],["causal-inference.html", "Module 3 Causal Inference 3.1 Core content 3.2 Slides 3.3 Resources", " Module 3 Causal Inference Much of social science is about causality: we may ask if say personal narratives of immigrants help reduce prejudicial attitudes towards them, or whether voter registration increases political participation or if bottom-up accountability can improve health outcomes. Even though we make causal claims all the time, not all are based on good comparisons. A good reseach design helps us establish whether there is a causal effect of a policy, action or program on a particular outcome. There is a long history of work on causality dating back to classic writing of Fisher (1935) and Rubin (1974). Over the past decade, with the increased use of experiments, social science has become a much more serious how such claims are made. Randomization has been embraced as the gold standard. In this module, we introduce the counterfactual approach to causal inference, and these causal claims can be interpreted. We introduce the potential outcome framework and introduce how random assignment helps us make claims about what would have happened without the policy, action or program we study. We then introduce the three core assumptions of any causal claim: randomization, non-interference and excludability. 3.1 Core content Introduce what do we mean when we say “cause”? (And why does it matter to be clear about the meaning of causal claims?) An introduction to potential outcomes as a way to think about alternative states of the world. Show how randomization helps us learn about counterfactual causal claims in a particularly useful way. Introduce the three key core assumptions for causal inference: random assignment of subjects to treatment, non-interference, excludability. Compare randomized versus observational studies. Randomization brings high internal validity, but it can’t promise external validity. Your causal question closely links so your Research Design. 3.2 Slides Below are slides with the core content that we cover in our lecture on causality. You can directly use these slides or make your own local copy and edit. R Markdown Source PDF Version HTML Version You can also see the slides used in previous EGAP Learning Days: The causal inference presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019 The causal inference presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018 The causal inference presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019 The causal inference presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017 The introduction to experiments presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017 The causal inference presentation from EGAP Learning Days in Salima, Malawi, February 2017 The causal inference presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016 3.3 Resources 3.3.1 EGAP Methods Guides EGAP Methods Guide 10 Things You Need to Know about Causal Inference EGAP Methods Guide 10 Strategies for Figuring Out If X Caused Y EGAP Methods Guide 10 Things You Need to Know about Mechanisms EGAP Methods Guide 10 Things to Know About External Validity 3.3.2 Books, Chapters, and Articles “Causation and Explanation in Social Science” (Brady 2008) Field Experiments: Design, Analysis, and Interpretation, Chapter 1 (Gerber and Green 2012). This book is a great resource for many topics in experimental design. Counterfactuals and Causal Inference: Methods and Principles for Social Research, Chapter 1 (Morgan and Winship 2007). This book includes nice examples of thinking through making causal claims from observational data. Running Randomized Evaluations: A Practical Guide (Glennerster and Takavarasha 2013). This is a great introduction with many examples of field studies for policy-makers. 3.3.3 EGAP Policy Briefs Some examples of causal questions: Does bottom-up, citizen-based monitoring improve public service delivery? Can bottom-up accountability generate improvements in health outcomes? Can free and anonymous information communication technology strengthen local accountability and improve the delivery of public services? Are radio voter education campaigns effective in discouraging voters from voting for parties/candidates that engage in vote-buying? "],["randomization.html", "Module 4 Randomization 4.1 Core content 4.2 Slides 4.3 Resources", " Module 4 Randomization Randomized controlled trials (RCTs) play a special role in how scholars about their theories and about how policy-makers learn about the success of their programs because of randomization. Although the basic idea is simple, we have made this module separate from others because randomization is central to the enterprise, to help you understand just what it is and why it might play the role that it does, and to provide some guidance about implementing it (and avoiding common mistakes). In this module we define randomization, separate random selection from random assignment, discuss common ways randomization is done and applied in designs. 4.1 Core content What is randomization? (Random assignment is not the same as random sampling.) Four common ways to randomize treatment: Simple: randomly assign units to treatment (like a coin flip) Complete: within a list of eligible units, a assign a fixed number to receive a treatment (like drawing from a urn) Cluster: assign groups or clusters of observations to the same treatment condition Block: assign treatment within specific strata or blocks (as if you are running an experiment within each block) Some commonly used designs: Randomized access: randomization to availability of a treatment Randomized delayed access: randomize the timing of access Factorial: randomize units to combinations of treatment arms Encouragement: randomize the invitation to receive treatment How do you check whether your randomization produced balance on observables? Typically we conduct randomization tests also known as balance tests using the \\(d^2\\) omnibus test from xBalance in the RItools package (because it is randomization inference) or approximate this result with an \\(F\\)-test. There are, of course, limits to randomization. We discuss some here and refer to the session on Threats for more. 4.2 Slides Below are slides with the core content that we cover in our lecture on randomization. You can directly use these slides or make your own local copy and edit. R Markdown Source PDF version HTML version The linked files shows how to do replicable randomization in R. You can also see more examples of randomization in R at 10 Things You Need to Know About Randomization. You can also see the slides used in previous EGAP Learning Days: The design issues presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019 (first section reviews randomization designs) The randomization presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018 The randomization presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019 The randomization presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017 The randomization presentation from EGAP Learning Days in Salima, Malawi, February 2017 The randomization presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016 4.3 Resources 4.3.1 EGAP Methods Guides EGAP Methods Guide 10 Things You Need to Know About Randomization EGAP Methods Guide 10 Things You Need to Know About Cluster Randomization 4.3.2 Books, Chapters, and Articles Standard operating procedures for Don Green’s lab at Columbia University. A comprhensive set of procedures and rules of thumb for conducting experimental studies. Running Randomized Evaluations: A Practical Guide, Chapter 4: Randomizing (Glennerster and Takavarasha 2013). Field Experiments: Design, Analysis, and Interpretation, Chapter 2: Causal Inference and Experimentation (Gerber and Green 2012). 4.3.3 EGAP Policy Briefs Factorial Design How Media Influence Social Norms: Evidence from Mexico Does Bottom-Up Accountability Work? Access Reducing Elite Capture in the Solomon Islands Delayed access Reducing Youth Support for Violence through Training and Cash Transfers in Afghanistan Reducing Reconvictions Among Released Prisoners Cluster design Getting Out the Vote Block and Cluster design Reporting Corruption Incumbent Malfeasance Revelations Encouragement design vote turn out Natural experiment (no randomization) Violent Conflict and Behavior in Burundi "],["hypothesis-testing.html", "Module 5 Hypothesis Testing 5.1 Core content 5.2 Slides 5.3 Resources", " Module 5 Hypothesis Testing If we cannot directly observe how a person or a village might react to both a treatment and a control intervention because of the fundamental problem of counterfactual causal inference (as described in the Causal Inference Module), how can we learn about cause and effect using what we observe? In a randomized experiment, we can assess guesses about the unobserved causal effects by comparing what we observe in a given experiment to what we would observe if we were able to repeat the experimental manipulation and the guess or claim or hypothesis were true. In this module we introduce hypothesis testing, how it relates to causal inference, what a hypothesis test is, and what to do when you have multiple hypothesis to test. 5.1 Core content What is a good hypothesis? What is the relationship between hypothesis testing and causal inference? Why test hypotheses when we want to learn about the causal effect of some intervention on some outcome? What is a hypothesis test? What is a null hypothesis? Estimators versus test statistics. In an experiment, a reference distribution for a hypothesis test comes from the experimental design and the randomization. What is a \\(p\\)-value? How should we interpret the results of hypothesis tests? What do we want from a hypothesis test? A good test casts doubt on the truth rarely (i.e., has a controlled and low false positive rate) A good test easily distinguishes signal from noise (i.e., casts doubt on falsehoods often; has high statistical power) How would we know when our hypothesis test is doing a good job? (Power analysis is its own module) What is a false positive rate? What is correct coverage of a confidence interval? (And why are we mentioning confidence intervals when we talk about hypothesis tests?) How might we assess the false positive rate of a hypothesis test for a given design and choice of test statistic? (The case of cluster-randomized trials and robust cluster standard errors.) Be careful when testing many hypotheses (for example, if you have more than two treatment arms or if you are assessing the effects of a treatment on multiple outcomes). The following kinds of questions leads to multiple comparisons: Does the effect of an experimental treatment differ between different groups? Could differences in treatment effect arise because of some background characteristics of experimental subjects? Which, among several, strategies for communication were most effective on a single outcome? Which, among several outcomes, were influenced by a single experimental intervention? For these questions, we have many tests/intervals, and we can easily mislead ourselves into thinking we have made a discovery when we have only rejected the null by chance. We should be careful to adjust the \\(p\\)-values or confidence intervals to reflect the number of tests/intervals produced. 5.2 Slides Below are slides with the core content that we cover in our lecture on hypothesis testing. You can directly use these slides or make your own local copy and edit. R Markdown Source PDF Version HTML Version You can also see the slides used in previous EGAP Learning Days: The hypothesis testing presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019 The hypothesis testing presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018 The hypothesis testing presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019 The hypothesis testing presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017 The hypothesis testing presentation from EGAP Learning Days in Salima, Malawi, February 2017 The hypothesis testing presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016 5.3 Resources 5.3.1 EGAP Methods Guides 10 Things to Know About Hypothesis Testing 10 Things You Need to Know about Multiple Comparisons 5.3.2 Books, Chapters, and Articles Gerber, Alan S., and Donald P. Green. 2012. Field Experiments: Design, Analysis, and Interpretation. New York, NY: W. W. Norton &amp; Company., Chapter 3: Sampling Distributions, Statistical Inference, and Hypothesis Testing Rosenbaum, P R. 2010. “Design of observational studies.” Springer Series in Statistics. New York [etc.]: Springer., Chapter 2: Causal Inference in Randomized Experiments "],["estimands-and-estimators.html", "Module 6 Estimands and Estimators 6.1 Core content 6.2 Slides 6.3 Resources", " Module 6 Estimands and Estimators In a randomized experiment, we can make guesses about the average outcome under treatment and the average outcome under control that are not systematically different from the truth. That is, we can write down unbiased estimators of the unobserved average treatment effect. Since averages are of widespread policy and scientific interest, the fact that randomized experiments allow us to make good guesses about them has encouraged the use of estimators of point effects, and the use of measures of how these estimators might vary from experiment to experiment in the form of standard errors and confidence intervals. In this module we introduce types of estimands for our hypothesis and how we can analyse experimental data using an estimator to estimate our estimand and noise (standard error, confidence intervals), and the role of covatiate adjustments. 6.1 Core content A causal effect, \\(\\tau_i\\), is a comparison of unobserved potential outcomes for each unit \\(i\\). For example, this can be a difference or a ratio of unobserved potential outcomes. To learn about \\(\\tau_{i}\\), we can treat \\(\\tau_{i}\\) as an estimand or target quantity to be estimated (this module) or as a target quantity to be hypothesized about (Hypothesis Testing Module). Many focus on the Average Treatment Effect (ATE), \\(\\bar{\\tau}=\\sum_{i=1}^n \\tau_{i}\\), in part, because it allows for easy estimation. An estimator is a recipe for calculating a guess about the value of an estimand. For example, the difference of observed means for \\(m\\) treated units is one estimator of \\(\\bar{\\tau}\\). Different randomizations will produce different values of the same estimator targeting the same estimand. A standard error summarizes this variability in an estimator. A \\(100(1-\\alpha)\\)% confidence interval is a collection of hypotheses that cannot be rejected at the \\(\\alpha\\) level. We tend to report confidence intervals containing hypotheses about values of our estimand and use our estimator as a test statistic. Estimators should (1) avoid systematic error in their guessing of the estimand (be unbiased); (2) vary little in their guesses from experiment to experiment (be precise or efficient); and perhaps ideally (3) converge to the estimand as they use more and more information (be consistent). Analyze as you randomize in the context of estimation means that (1) our standard errors should measure variability from randomization and (2) our estimators should target estimands defined in terms of potential outcomes. We do not control for background covariates when we analyze data from randomized experiments. But covariates can make our estimation more precise. This is called covariance adjustment. Covariance adjustment in randomized experiments differs from controlling for in observational studies. One can create a design that aims to assess whether causal effects vary by subgroup using blocking or by pre-registering subgroup based analyses (for example, comparing the average effect among group A with the average effect among group B). A policy intervention (like a letter encouraging exercise) may intend to change behavior via an active dose (actual exercise). We can learn about the causal effect of the intention by randomly assigning letters (this is the Intent to Treat Effect). We can learn about the causal effect of actual execise by using the random assignment of letters as an instrument for the Active dose (exercise itself) in order to learn about the causal effect of exercise among those who would change their behavior after reading the letter (the average causal effect versions of these effects are often known as the Complier Average Causal Effect or the Local Average Treatment Effect). 6.2 Slides Below are slides with the core content that we cover in this session. At the moment, we do not cover block randomization, cluster randomization, binary outcomes, or covariate adjustment. Please refer to Chapter 3 in Gerber and Green (2012) and see the links and citations in the estimatr package for R. R Markdown Source PDF Version HTML Version You can also see the slides used in previous EGAP Learning Days: The estimation presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019 The estimation presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018 The estimation presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019 The estimation presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016 You can also discussion of the problems of estimating the effect of the active dose of a treatment in these slides (as well as discussion of the problems that missing data on outcomes cause for estimation of average causal effects): The design issues presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019 (first section reviews randomization designs) The spillovers and attrition presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017 The threats presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017 The complications presentation from EGAP Learning Days in Salima, Malawi, February 2017 The threats presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016 (the middle section reviews ITT and non-compliance ) 6.3 Resources 6.3.1 EGAP Methods Guides EGAP Methods Guide 10 types treatment effect you should know about EGAP Methods Guide 10 things to know about covariate adjustment EGAP Methods Guide 10 Things to Know about Missing Data EGAP Methods Guide10 Things to Know about the Local Average Treatment Effect (effects on Compliers and Non-compliance) EGAP Methods Guide 10 Types of Treatment Effect You Should Know About EGAP Methods Guide 10 Things to Know about Spillovers 6.3.2 Books, Chapters, and Articles Gerber and Green (2012), Chapter 2.7 (on Excludability and Non-interference), Chapter 3, Chapter 5 on One-sided noncompliance, Chapter 6 on Two-Sided Noncompliance, Chapter 7 on Attrition, Chapter 8 on Interference between Experimental Units Bowers and Leavitt (2020) 6.3.3 Tools DeclareDesign estimatr package for R "],["statistical-power-and-design-diagnosands.html", "Module 7 Statistical Power and Design Diagnosands 7.1 Core Content 7.2 Slides 7.3 Resources", " Module 7 Statistical Power and Design Diagnosands Before we run a study, we would like to know whether a particular design has the statistical power to detect an effect if it exists. It is difficult to learn from an under-powered study and a power analysis can help you improve your design or even decide against conducting the study. In this module, we introduce statistical power, core approaches to calculating power analytically or through simulation, and show how design features such as blocking, covatiate adjustment and clustering impact on power. 7.1 Core Content Statistical power is the ability of a study to detect an effect given that it exists. Power analysis is something we do before a study as it helps you figure out the sample you need, or what effects you can detect. It is an essential step in research design and helps you communicate about your design. Common approaches to power calculation: Analytical power calculations (using a formula) Using simulations (for example using DeclareDesign) Covariate adjustment and blocking can increase power. For clustered designs you need to take account of the intra-cluster correlation (the within cluster variance relative to the overall variance). Power is closely liked to your study design, hypothesis testing and estimation See also the glossary of terms. 7.2 Slides Below are slides with the core content that we cover in our lecture on power. You can directly use these slides or make your local copy and edit. R Markdown Source PDF version HTML version You can also see the slides used in previous EGAP Learning Days: The power presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019 The power presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019 The power presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018 The power presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017 The power presentation from EGAP Learning Days in Salima, Malawi, February 2017 The power presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016 7.3 Resources 7.3.1 EGAP Methods Guide EGAP Methods Guide10 Things to Know About Statistical Power EGAP Methods Guide 10 Things to Know about Covariate Adjustment EGAP Methods Guide 10 Things Your Null Results Might Mean 7.3.2 EGAP Policy Briefs and PAPs Some examples of power analysis in designs: Pre-Analysis Plan. Accountability Can Transform (ACT) Health: A Replication and Extension of Bjorkman and Svensson (2009) EGAP Policy Brief 58: Can bottom-up accountability generate improvements in health outcomes? 7.3.3 Tools Interactive power analysis EGAP Power Calculator rpsychologist R-Packages for power analysis pwr DeclareDesign, see also https://declaredesign.org/ "],["measurement.html", "Module 8 Measurement 8.1 Core content 8.2 Slides 8.3 Resources", " Module 8 Measurement To measure an outcome of interest and test hypothesis, we often use quantitative data derived from surveys, games or administrative records. For causal questions we typically use data on immediate and final outcomes and core mechanisms. We use baseline data to identify relevant subgroups, adjust our estimates or help block randomize our treatment. Data can be noisy (random error) and biased (systematic error). Measurement should be valid and reliable. In this module we discuss what to measure and how to measure and show how good measurement is closely linked to your research design and statistical power. 8.1 Core content When we represent some attribute of unit by some number, letter, word, symbol in some systematic way (perhaps in a cell in a simple dataset), we are measuring. A valid measure of a concept or phenomenon of interest should clearly represent that underlying and often abstract entity. For example, does question X measure “math ability?” It does so well, at first glance, if a community of scholars all agrees that people with higher values of question X or test Y do indeed have higher math ability. A reliable measure of a concept would provide the same score for the unit of measurement (a person, a village) if conditions were not changed. For example, an unreliable measure of the concept of “length” might be a rubber meter stick used by a 5 year old (who sometimes thinks of it as a sword). We can assess our theories of measurement using: manipulation checks of the treatment variables; and also multiple, different approaches to measuring outcomes; covariates; or differences between units implied by different accounts of causal mechanisms. (Sometimes measurement is so hard that a pilot study focusing on measurement is called for.) Invalid measurement can make it hard for your research design to effectively counter alternative explanations for the relationship between treatment and outcome. Unreliable measurement can diminish statistical power. 8.2 Slides Below are slides with the core content that we cover in our lecture on measurement. You can directly use these slides or make your own local copy and edit. R Markdown Source PDF Version HTML Version 8.3 Resources 8.3.1 EGAP Methods Guides EGAP Methods Guide 10 Things to Know about Measurement in Experiments EGAP Methods Guide 10 Things to Know About Survey Design EGAP Methods Guide 10 Things to Know About Survey Implementation 8.3.2 Books, Chapters, and Articles Adcock, Robert, and David Collier. 2001. “Measurement Validity: A Shared Standard for Qualitative and Measurement Validity: A Shared Standard for Qualitative and Quantitative Research.” American Political Science Review 95 (3): 529–46. Scacco, Alexandra, and Shana S. Warren. 2018. “Can Social Contact Reduce Prejudice and Discrimination? Evidence from a Field Experiment in Nigeria.” American Political Science Review 112 (3): 654–77 Shadish, William R, Thomas D Cook, Donald Thomas Campbell, and others. 2002. Experimental and Quasi-Experimental Designs for Generalized Causal Inference/William R. Shedish, Thomas d. Cook, Donald T. Campbell. Boston: Houghton Mifflin Vicente, Pedro C. 2014. “Is Vote Buying Effective? Evidence from a Field Experiment in West Africa.” Economic Journal 124 (574): F356–87 8.3.3 EGAP Policy Briefs Examples of measurement strategies: Survey data at multiple levels EGAP Policy Brief 58: Does Bottom-Up Accountability Work? Text messages EGAP Policy Brief 27: ICT and Politicians in Uganda EGAP Policy Brief 56: Reporting Corruption in Nigeria Administrative data EGAP Policy Brief 67: Electoral Administration in Kenya EGAP Policy Brief 16: Spillover Effects of Observers in Ghana "],["threats-to-internal-validity-of-randomized-experiments.html", "Module 9 Threats to internal validity of randomized experiments 9.1 Core content 9.2 Slides 9.3 Resources", " Module 9 Threats to internal validity of randomized experiments Randomized experiments can run into issues that can threaten the internal validity of randomized experiments. Some units might be missing outcome data and that missingness may be due to the treatment. They may not take the treatment status assigned to them or be subject to spillover effects from a treated neighbor. In this module, we covers some common threats and some best practices to avoid or work around them. 9.1 Core content Review the three core assumptions discussed in the Causal Inference Module. We have said “Analyze as you randomize” elsewhere (Estimands and Estimators Module). Remember that you randomized treatment assignment, not receiving treatment or participating in data collection. Missing data on the outcome (attrition) is especially a problem if the patterns of missingness are caused by the treatment itself. This is the most common problem. Do not drop observations that are missing outcome data from your analysis. You may be able to bound treatment effect estimates. Non-compliance. The effect of treatment assignment is not the same as the effect of receiving the treatment. Sometimes units will not comply with their assigned treatment status, but not all hope is lost. One-sided compliance when some units assigned to treatment fail to take the treatment, but all units assigned to control do not take the treatment. The “local average treatment effect” (LATE, also known as the “complier average causal effect,” CACE) is the average effect for the units who take the treatment when assigned, but not otherwise. If the monotonicity assumption and the exclusion restriction hold, we may be able to estimate LATE. “Spillover effects” or interference between units is a violation of one of the core assumptions for causal inference (Causal Inference). However, this may not be a problem if you are interested in spillover effects and/or have designed your research to account for it. Hawthorne effects are when subjects behave differently because they are being observed. Non-excludability Treating treatment and control units differently, such as with different data collection processes or extra attention to the treated units, can be confuse interpretation of experimental results. If Hawthorne effects are present for treated units but not control units, then we have a violation of the excludability assumption. 9.2 Slides Below are slides with the core content that we cover in our lecture on threats. You can directly use these slides or make your local copy and edit. R Markdown Source PDF version HTML version You can also see the slides used in previous EGAP Learning Days: The attrition and missing data presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019 The threats presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019 (first section reviews randomization designs) 9.3 Resources 9.3.1 EGAP Methods Guide EGAP Methods Guide 10 Things to Know About Missing Data EGAP Methods Guide 10 Types of Treatment Effect You Should Know About EGAP Methods Guide 10 Things to Know About The Local Average Treatment Effect 9.3.2 Books, Chapters, and Articles Standard operating procedures for Don Green’s lab at Columbia University. A comprhensive set of procedures and rules of thumb for conducting experimental studies. 9.3.3 EGAP Policy Briefs EGAP Policy Brief 16: Spillover Effects of Observers in Ghana EGAP Policy Brief 11: Election Observers and Fraud in Ghana "],["ethical-considerations.html", "Module 10 Ethical Considerations 10.1 Core content 10.2 Slides 10.3 Resources", " Module 10 Ethical Considerations Experiments involve intervention. A research team (composed of well-meaning members from universities, governments, and other organizations) has reason to believe that a new invention will improve the lives of people and also hope to learn about the causal effect of the intervention clearly enough to inform the theories of change that justify this and other interventions in this place and elsewhere. In an experiment, interventions are randomly assign and we compare the lives of people (their behavior, their attitudes, etc.) between the status quo and the new intervention. Notice that such an experiment involves one group of humans changing the lives of another group humans. When one person influences the life of another the influencer has responsibilities not to harm the person being influenced. Folks working in government do this as a matter of course — their very job is to provide food, shelter, safety, justice, etc.. to their publics. Randomized experiments help governmental actors learn more quickly about what works and what does not. Academics also have direct effects on their students, but their effects on the public usually is indirect. When academics and policy makers collaborate the effects on the public can be immediate. So, even if academics usually spend little time worrying about negative effects on the public, a randomized experiment is one moment where they should spend more time on this worry. This module discusses the core ethical topics, such as privacy and autonomy, and the basic principles relating to respect for persons, beneficence and justice, and how informed consent, helps communicate about these principles to study participants. This module reminds research teams to carefully assess how the intervention might change the lives of those exposed to it (as well as those not exposed) so that everyone proceeds with the experiment feeling sure that it will do no harm. 10.1 Core content Research must weigh the potential benefits from the knowledge to be gained from the research against the potential harms to human subjects How would you feel if you were a research subject in your study? In the control group? In the treatment group? A relatively high status member of the community? A relatively low status member of the community? Key tenets: privacy and autonomy Basic Principles in the Belmont Report: respect for persons, beneficence, justice Informed consent: Can you ensure that research subjects have the freedom to refuse to participate and/or drop out of the study if they feel they should? Challenges for social science experimental research in general: many more people may benefit (or suffer from) your intervention than directly participate in your study. changing elections or corruption can produce large societal changes. Is this beyond the remit of research? 10.2 Slides Below are slides with the core content that we cover in this session. R Markdown Source PDF Version HTML Version 10.3 Resources EGAP Research Principles and Work on Ethics Belmont Report Institutional Review Boards in the US Example: Research Ethics at Oxford University in the UK Example: Ethics for Researchers in the EU Example: Research Ethics at the Universidad Catolica de Chile 10.3.1 Books, Chapters, and Articles Asiedu, Edward, Dean Karlan, Monica P Lambon-Quayefio, and Christopher R Udry. 2021. “A Call for Structured Ethics Appendices in Social Science Papers.” Working Paper 28393. Working Paper Series. National Bureau of Economic Research. doi:10.3386/w28393. [Evans, David K. 2021. “Towards Improved and More Transparent Ethics in Randomised Controlled Trials in Development Social Science.” Working Paper 565. Center for Global Development. https://www.cgdev.org/sites/default/files/WP565-Evans-Ethical-issues-and-RCTs.pdf.(Evans (2021)) 10.3.2 EGAP Policy Briefs and PAPs Examples of PAPs and Papers that discuss ethical issues: Pre Analysis Plan: The Effects of Non-Food Item Vouchers in a Humanitarian Context The Case of the Rapid Response to Movements of Population Program in Congo Paper: Appendix E.1 in Countering violence against women by encouraging disclosure: A mass media experiment in rural Uganda "],["glossary-of-terms.html", "Module 11 Glossary of Terms 11.1 Key Concepts 11.2 Statistical Inference 11.3 Randomization Strategies 11.4 Factorial Designs 11.5 Threats", " Module 11 Glossary of Terms Below are some core terms frequently used throughout the book and more broadly in discussions of randomized field experiments. 11.1 Key Concepts See the module on causal inference, estimands and estimators. Potential outcome \\(Y_i(Z)\\) What outcome \\(Y\\) that unit \\(i\\) would have under treatment condition \\(Z\\). We think of these as fixed quantities. Z can be 0 (for control) or 1 (for treatment). See the module on causal inference. Treatment effect \\(\\tau_i\\) for unit \\(i\\) The difference between potential outcomes under treatment and control, \\(Y_i(1)-Y_i(0)\\). See the module on causal inference. Fundamental Problem of Counterfactual Causal Inference We can’t observe \\(Y_i(1)\\) and \\(Y_i(0)\\) for the same unit at the same time, so we can’t get \\(\\tau_i\\) directly. See the module on causal inference. Estimand The thing you want to estimate. Example: average treatment effect. In counter-factual causal inference, this is a function of potential outcomes, not fully observed outcomes. See the module on estimands and estimators. Estimator How you make a guess about the value of your estimand from the data you have. Example: difference-in-means. See the module on estimands and estimators. Average treatment effect, ATE The average of the treatment effect for all individuals in your subject pool. This is an estimand. \\(\\overline{Y_i(1)-Y_i(0)}\\), which is also equivalent to \\(\\bar{Y}_i(1)-\\bar{Y}_i(0)\\). Notice that we do not use the \\(E[Y_i (1)]\\) style of notation here because \\(E[]\\) means “average over repeated operations” but \\(\\bar{Y}\\) means “average over a set of observations.” See the module on causal inference and the module on estimands and estimators. Random Sampling Selecting subjects from a population with known probabilities (strictly between 0 and 1). \\(k\\)-Arm Experiment An experiment that has \\(k\\) treatment conditions (including control). See the module on randomization. Random Assignment Assigning subjects with known probability (without replacement) strictly between 0 and 1 to experimental conditions. This is the same as random sampling from the potential outcomes. There are several strategies for random assignment: simple, complete, cluster, block, blocked cluster. See the module on randomization. External Validity Findings from your study teach you about contexts outside of your sample — other locations or in other interventions. 11.2 Statistical Inference See module on hypothesis testing and statistical power. Hypothesis Simple, clear, falsifiable claim about the world. In counter-factual causal inference, this is a statement about a relationship among potential outcomes, like \\(H_0: Y_i(Z_i=0) = Y_i(Z_i=1) + \\tau_i\\) for the hypothesis that the potential outcome to treatment is produced from the potential outcome to control plus some effect for each unit \\(i\\). See the module on hypothesis testing. Null Hypothesis A conjecture about the world that you may reject after seeing the data. See the module on hypothesis testing. Sharp Null Hypothesis of No Effect The null hypothesis that there is no treatment effect for any subject. This means \\(Y_i(1)=Y_i(0)\\) for all \\(i\\). We might write this, \\(H_0: Y_i(Z_i=0) = Y_i(Z_i=1)\\). See the module on hypothesis testing. \\(p\\)-value The probability seeing a test statistic as large (in absolute value) as the observed test statistic (e.g., the difference in means) or larger. See the module on hypothesis testing. One-sided vs.~Two-sided Test When you have a strong expectation that the effect is either positive or negative, you can conduct a one-sided test. When you do not have such a strong expectation, conduct a two-sided test. A one-sided test has more power than a two-sided test for the same experiment. See the module on hypothesis testing. Standard Deviation Square root of the mean-square deviation from average of a variable. It is a measure of the dispersion or spread of a statistic. \\(SD_x=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_i-\\bar{x})^2}\\) False Positive Rate/Type I Error of a Test A well-operating hypothesis test rejects a hypothesis about a true causal effect no more than \\(\\alpha\\) % of the time. The false positive rate is the rate at which a test will cast doubt on a true hypothesis, the rate at which the test will encourage the analyst to say “statistically significant” when, in fact, there is no causal relationship. See the module on hypothesis testing. Sampling Distribution The distribution of estimates (e.g., estimates of the ATE) for all possible treatment assignments. In design-based statistical inference for randomized experiments, the distribution of estimates from an estimator is generated from randomizations. Many call this a “sampling distribution” because textbooks often use the idea of repeated samples from a population rather than repeated randomizations to describe this kind of variation. Standard Error The standard deviation of the sampling distribution. A bigger standard error means that our estimates are more susceptible to sampling variation. See the module on estimands and estimators. Coverage of a confidence interval A well-operating confidence interval contains the true causal effect \\(100 ( 1 - \\alpha)\\) % of the time. A confidence interval that has incorrect coverage when it excludes the true parameter less than \\(100 (1 - \\alpha)\\)% of the time (for example, a 95% confidence interval is supposed to only exclude the true parameter less than 5% of the time). Statistical Power of a Test Probability that a test of causal effects will detect a statistically significant treatment effect if the effect exists. See the module on statistical power. This depends on: The number of observations in each arm of the experiment Effect size (usually measured in standardized units) Noisiness of the outcome variable Significance level (\\(\\alpha\\), which is fixed by convention) Other factors including what proportion of your units are assigned to different treatment conditions. Intra-Cluster Correlation How correlated the potential outcomes of units are within clusters compared to across clusters. Higher intra-cluster correlation hurts power. Unbiased An estimator is unbiased if you expect that it will return the right outcome. That means that if you were to run the experiment many times, the estimate might be too high or to low sometimes but it will be right on average. See the module on estimands and estimators. Bias Bias is the difference between the average value of the estimator across its sampling distribution and the single, fixed value of the estimand. See the module on estimands and estimators. Consistency of an estimator An estimator that produces answers that become ever nearer the true estimand value as the sample size increases is a consistent estimator of that estimand. A consistent estimator may or may not be unbiased. See the module on estimands and estimators. Precision/Efficiency of an estimator The variation in or width of the sampling distribution of an estimator. See the module on estimands and estimators. 11.3 Randomization Strategies See the module on randomization. Simple An independent coin flip for each unit. You are not guaranteed that your experiment will have a specific number of treated units. Complete Assign \\(m\\) out of \\(N\\) units to treatment, i.e., you know how many units will be treated in your experiment. Each unit has a \\(m/N\\) probability of being treated. The number of ways treatment can be assigned (number of permutations of treatment assignment) is \\(\\frac{N!}{m!(N-m)!}\\). Block First divide the sample into blocks, then complete randomization in each block separately. A block is a set of units within which you conduct random assignment. Cluster Clusters of units are randomly assigned to treatment conditions. A cluster is a set of units that will always be assigned to the same treatment status. Blocked Cluster First form blocks of clusters. Then in each block, randomly assign the clusters to treatment conditions using complete randomization. 11.4 Factorial Designs See the module on randomization. Factorial Design A design with more than one treatment, with each treatment assigned independently. The simplest factorial design is a 2 by 2. Conditional Marginal Effect The effect of one treatment, conditional on the other being held at a fixed value. For example: \\(Y_i(Z_1=1|Z_2=0)-Y_i(Z_1=0|Z_2=0)\\) is the marginal effect of \\(Z_1\\) conditional on \\(Z_2=0\\). Average Marginal Effect Main effect of each treatment in a factorial design. It is the average of the conditional marginal effects for all the conditions of the other treatment, weighted by the proportion of the sample that was assigned to each condition. Interaction Effect In a factorial design, we may also estimate interaction effects. No interaction effect: one treatment does not amplify or undercut the effect of the other treatment. Multiplicative interaction effect: the effect of one treatment depends on whether a unit was assigned the other treatment. This means one treatment does amplify or undercut the effect of the other. The effect of two treatments together is not the sum of the effect of each treatment. 11.5 Threats See the module on more elaborate questions and the module on threats. Hawthorne Effects When a subject responds to being observed. Spillovers When a subject responds to another subject’s treatment status. Example: my health depends on whether my neighbor is vaccinated, as well as whether I am vaccinated. Attrition When outcomes for some subjects are not measured. Example: people migrate or people die. This is especially problematic for inference when correlated with treatment status. Compliance A unit’s treatment status matches its assigned treatment condition. Example of non-compliance: a unit assigned to treatment doesn’t take it. Example of compliance: a unit assigned to control does not take treatment. Compliance Types There are four types of units in terms of compliance: Compliers Units that would take treatment if assigned to treatment and would be untreated if assigned to control. Always-Takers Units that would take treatment if assigned to treatment and if assigned to control. Never-Takers Units that would be untreated if assigned to treatment and if assigned to control. Defiers Units that would be untreated if assigned to treatment and would take treatment if assigned to control. One-sided Non-Compliance The experiment has only compliers and either always takers or never takers. Usually, we think of one-sided non-compliance as having only never takers and compliers meaning that that local average treatment effect is the effect of treatment on the treated. Two-sided Non-Compliance The experiment may have all four latent groups. Encouragement Design An experiment that randomizes \\(Z\\) (treatment assignment), and we measure \\(D\\) (whether the unit takes treatment) and \\(Y\\) (outcome). We can estimate the ITT and the LATE (Local Average Treatment Effect, aka CACE—Complier Average Causal Effect). It requires three assumptions. Monotonicity Assumption of either no defiers or no compliers. Usually we assume no defiers which means that the effect of assignment on take up of treatment is either positive or zero but not negative. First Stage Assumption that there is an effect of \\(Z\\) on \\(D\\). Exclusion Restriction Assumption that \\(Z\\) affects \\(Y\\) only through \\(D\\). This is usually the most problematic assumption. Intention-to-Treat Effect (ITT) The effect of \\(Z\\) (treatment assignment) on \\(Y\\). Local Average Treatment Effect (LATE) The effect of \\(D\\) (taking treatment) on \\(Y\\) for compliers. Also known as Complier Average Causal Effect (CACE). Under the exclusion restriction and monotonicity, the LATE is equal to ITT divided by the proportion of your sample who are Compliers. Downstream Experiment An encouragement design study that takes advantage of the randomization of \\(Z\\) by a previous study. The outcome from that previous study is the \\(D\\) in the downstream experiment. "],["introduction-to-r-and-rstudio.html", "Module 12 Introduction to R and RStudio 12.1 R and RStudio 12.2 Downloading R and RStudio 12.3 RStudio Interface 12.4 Learning to Use R", " Module 12 Introduction to R and RStudio Throughout the book we include R code for estimation, similation, and creating examples. We used RStudio to create the slides. To personalize them for your own purpose, we assume you will use R+markdown. Below, we include guide on setting up R and RStudio on your machine, as well as some basic commands often used. 12.1 R and RStudio R is an free software environment most commonly used for statistical analysis and computation. Because Learning Days participants arrive with different statistical backgrounds and preferred statistical software, we will use R to ensure that everyone is on the same page. We advocate the use of R more generally for its flexibility, wealth of applications, and comprehensive online support (mostly forums). RStudio is a free, open source integrated development environment for with an user interface that makes R much more user-friendly. R Markdown, a feature of RStudio, enables the easy output of code, results, and text in a .pdf, .html, or .doc format. This document provides a tutorial on downloading R and RStudio in addition to an introduction to the interface. 12.2 Downloading R and RStudio 12.2.1 Downloading R R can be freely downloaded from CRAN at the link corresponding to your operating system: For Windows: https://cran.r-project.org/bin/windows/base/ For Mac OS X: https://cran.r-project.org/bin/macosx/. Select R-3.4.3.pkg for OS X 10.11 and higher. Select R-3.3.3.pkg for OS X 10.9-10.11. Select R-3.2.1-snowleopard.pkg for OS X 10.6-10.8. 12.2.2 Downloading RStudio RStudio can be freely downloaded from the RStudio website, https://www.rstudio.com/products/rstudio/download/. In the table, click the green Download button at the bottom of the left column, “RStudio Desktop Open Source License” as depicted below in Figure 1. Once you select this button, the page will jump to a list of download options as depicted in Figure 2 (page 3). For Windows, select Windows Vista/7/8/10. For Mac OS X, select Mac OS X 10.6+ (64-bit). knitr::include_graphics(here(&quot;Resources/Images&quot;,&quot;new_rstudio.png&quot;)) Figure 12.1: Select Download in the “RStudio Desktop Open Source License” column. knitr::include_graphics(&quot;Resources/Images/rstudio_download.png&quot;) Figure 12.2: Select the Windows Vista/7/8/1 link for Windows or the Mac OS X 10.6+ (64-bit) link for Mac. 12.3 RStudio Interface When you open RStudio for the first time, there should be three panels visible, as depicted in Figure 3 below. Console (left panel) Accounting (upper right panel): includes Environment and History tabs Miscellaneous (lower right panel) knitr::include_graphics(&quot;Resources/Images/rstudio_intro.png&quot;) Figure 12.3: When you open RStudio, there are three panels visible: the Console (left), Accounting (upper right), and Miscellaneous (lower right). 12.3.1 Console One can execute all operations in the console. For example if one entered 4 + 4 and hit the Enter/Return key, the console will return [1] 8. To make sure everyone is prepared to use R at Learning Days, we ask you to run one line of code in the Console to download several R packages. Packages are fragments of reproducible code that allow for more efficient analysis in R. To run these lines, copy the following code into the console and hit your Return/Enter key. Note that you must be connected to the internet to download packages. install.packages(c(&quot;ggplot2&quot;, &quot;dplyr&quot;, &quot;AER&quot;, &quot;arm&quot;, &quot;MASS&quot;, &quot;sandwich&quot;, &quot;lmtest&quot;, &quot;randomizr&quot;, &quot;DeclareDesign&quot;)) If successfully downloaded, your console will resemble Figure 4, though the urls will be different as a function of your location. Figure 12.4: An image of the console after executing the three lines of code listed above. 12.3.2 Editor In order to write and save reproducible code, we will open a fourth panel, the Editor, by clicking on the icon with a white page with a plus sign on the upper-left corner of the RStudio interface and selecting R Script, as depicted in Figure 5. knitr::include_graphics(&quot;Resources/Images/new_script.png&quot;) Figure 12.5: Create a new R script and open the editor panel by selecting R Script from the dropdown menu. Once the R script is opened, there should be four panels within the RStudio interface, now with the addition of the Editor panel. We can execute simple arithmetic by entering a formula in the editor and pressing Control + Enter (Windows) or Command + Enter (Mac). The formula and the “answer” will appear in the Console, as depicted in Figure 6 (page 6), with red boxes for emphasis. knitr::include_graphics(&quot;Resources/Images/first_addition.png&quot;) Figure 12.6: An arithmetic expression is entered in the editor and evaluated in the console. The red boxes are added for emphasis. R can be used for any arithmetic operation, including but not limited to: addition (+), subtraction (-), scalar multiplication (*), division (/), or exponentiation (^). 12.3.3 Accounting Beyond basic functions, we can also store values, data, and functions in the global environment. To assign a value to a variable, use the &lt;- operator. All stored values, functions, and data will appear in the environment tab in the Accounting panel. In Figure 7, we define the variable t to take the value \\(3 \\times \\frac{6}{14}\\), and can see that it is stored under Values. We also load a dataset. Here, “ChickWeight” is a dataset built into R; most datasets will be loaded from the web or other files on your computer through an alternate method. We can see that ChickWeight contains 578 observations of 4 variables and is stored in the Environment. By clicking on the name ChickWeight a tab will enter with the dataset in your Editor window. Figure 12.7: The value 3 * (6/14) is assigned to the variable t (red) and the dataset ChickWeight is added to the global environment (blue). The boxes are added for emphasis. R provides many tools to analyze and view data that we will discuss in more depth at Learning Days. For now, we can learn some basic tools to examine the data. The function head() allows us to see the first six rows of the dataset. summary() summarizes each of the columns of the dataset and dim() provides the dimensions of the dataset in terms of the number of rows and then columns. head(ChickWeight) # First 6 observations in dataset weight Time Chick Diet 1 42 0 1 1 2 51 2 1 1 3 59 4 1 1 4 64 6 1 1 5 76 8 1 1 6 93 10 1 1 summary(ChickWeight) # Summary of all variables weight Time Chick Diet Min. : 35 Min. : 0.0 13 : 12 1:220 1st Qu.: 63 1st Qu.: 4.0 9 : 12 2:120 Median :103 Median :10.0 20 : 12 3:120 Mean :122 Mean :10.7 10 : 12 4:118 3rd Qu.:164 3rd Qu.:16.0 17 : 12 Max. :373 Max. :21.0 19 : 12 (Other):506 dim(ChickWeight) # Dimensions of the dataset in the order rows, columns [1] 578 4 Unlike other statistical software, R allows for the storage of multiple datasets, possibly of different dimensions, simultaneously. As such it is quite flexible for analysis using multiple methods. 12.3.4 Miscellaneous R provides a suite of tools, ranging from built-in plot functions to packages to graph data, models, and estimates, etc. The final “miscellaneous” panel allows for viewing of graphs with ease in RStudio. Figure 8 depicts a plot in this panel. We will discuss how to plot data during Learning Days; for now, don’t worry about the graphing the code in the Editor. knitr::include_graphics(&quot;Resources/Images/graph.png&quot;) Figure 12.8: An example plot of the ChickWeight data made in R. 12.4 Learning to Use R 12.4.1 Online Resources There are many helpful online resources to help you start learning R. We recommend two sources: Code School, which runs entirely through your browser https://www.codeschool.com/courses/try-r. Coursera, via an online R Programming course organized by Johns Hopkins University: Go to coursera.org Create an account (this is free!) Sign up for R Programming at Johns Hopkins University (instructor: Roger Peng) under the “Courses” tab Read the materials and watch the videos from the first week. The videos from the first week are about 2.5 hours long total. 12.4.2 Basic Practice Here we provide some fragments of code to familiarize you with some basic practices in R. We recommend that you practice by typing the code fragments into your Editor and then evaluating them. 12.4.2.1 Setting up an R Session In general, we read other files such as data or functions into R and output results like graphs or tables into files not contained within an R session. To do this, we must give R an “address” at which it can locate such files. It may be most efficient to do this by setting a working directory, a file path at which relevant files are stored. We can identify the current working directory using getwd() and set a new one using setwd(). Note that the syntax of these filepaths varies by operating system. getwd() [1] &quot;/Users/miriamagolden/Dropbox/admin/external/egap/learningdays-guide-master/Guide&quot; setwd(&quot;~TaraLyn/Dropbox/EGAP Learning Days Admin/Workshop 2018_2 (Uruguay)/&quot;) You may need to install packages beyond those listed above to execute certain functions. To install packages we use install.packages(\"\"), filling in the package name between the \"\" marks, as follows. You need only install packages once. install.packages(&quot;Hmisc&quot;) Once a package is downloaded, it can be loaded and accessed using library() where the package name is inserted between the parentheses (no \"\" marks). library(Hmisc) To clear R’s memory, namely the stored data, functions, or values that appear in the accounting tab, use rm(list = ls()). It may be useful to set a random number seed to ensure that replication is possible, particularly when we work with simulation-based methods. rm(list = ls()) set.seed(2018) # Optional: Set a seed to make output replicable 12.4.2.2 R Basics We now explore some of the basic commands. In order to assign a scalar (single element) to a variable, we use the &lt;- command as discussed previously: (a &lt;- 5) # &quot;&lt;-&quot; is the assignment command; it is used to define things. eg: [1] 5 We may also want to assign a vector of elements to a variable. Here we use the same &lt;- command, but focus on how to create the vector. (b &lt;- 1:10) # &quot;:&quot; is used to define a string of integers [1] 1 2 3 4 5 6 7 8 9 10 (v &lt;- c(1, 3, 2, 4, pi)) # use c() to make a vector with anything in it [1] 1.000 3.000 2.000 4.000 3.142 We can then refer to elements of a vector by denoting their position in a vector inside hard brackets []. # Extract elements of a vector: b[1] # Returns position 1 [1] 1 b[5:4] # Returns positions 5 and 4, in that order [1] 5 4 b[-1] # Returns all but the first number [1] 2 3 4 5 6 7 8 9 10 # Returns all numbers indicated as &quot;TRUE&quot; b[c(TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE)] [1] 1 3 6 7 # Assign new values to particular elements of a vector b[5] &lt;- 0 There are a set of built-in functions that can be applied to vectors like b. sum(b) # Sum of all elements [1] 50 mean(b) # Mean of all elements [1] 5 max(b) # Maximum of all elements [1] 10 min(b) # Minimum of all elements [1] 0 sd(b) # Standard deviation of all elements [1] 3.496 var(b) # Variance of all elements [1] 12.22 We can also apply arithmetic transformations to all elements of a vector: b^2 # Square the variable [1] 1 4 9 16 0 36 49 64 81 100 b^.5 # Square root of the variable [1] 1.000 1.414 1.732 2.000 0.000 2.449 2.646 2.828 3.000 3.162 log(b) # Log of variable [1] 0.0000 0.6931 1.0986 1.3863 -Inf 1.7918 1.9459 2.0794 2.1972 2.3026 exp(b) # e to the b [1] 2.718 7.389 20.086 54.598 1.000 403.429 1096.633 2980.958 8103.084 22026.466 Finally, we can evaluate logical statements (i.e. ``is condition X true?’’) on all elements of a vector: b == 2 # Is equal to [1] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE b &lt; 5 # Less than [1] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE b &gt;= 5 # Greater than or equal to [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE b &lt;= 5 | b / 4 == 2 # | means OR [1] TRUE TRUE TRUE TRUE TRUE FALSE FALSE TRUE FALSE FALSE b&gt;2 &amp; b&lt;9 # &amp; means AND [1] FALSE FALSE TRUE TRUE FALSE TRUE TRUE TRUE FALSE FALSE is.na(b) # Indicates if data is missing [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE which(b&lt;5) # Gives indices of values meeting logical requirement [1] 1 2 3 4 5 The basic logic of these commands applies to data structures much more complex than scalars and vectors. Understanding of these basic features will help facilitate your understanding of more advanced topics during Learning Days. "],["references.html", "References", " References Bowers, Jake, and Thomas Leavitt. 2020. “Causality &amp; Design-Based Inference.” In The SAGE Handbook of Research Methods in Political Science and International Relations, edited by Luigi Curini and Robert Franzese. Sage Publications Ltd. Brady, Henry E. 2008. “Causation and Explanation in Social Science.” In The Oxford Handbook of Political Science. https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199286546.001.0001/oxfordhb-9780199286546-e-10. Evans, David K. 2021. “Towards Improved and More Transparent Ethics in Randomised Controlled Trials in Development Social Science.” Working Paper 565. Center for Global Development. https://www.cgdev.org/sites/default/files/WP565-Evans-Ethical-issues-and-RCTs.pdf. Gerber, Alan S., and Donald P. Green. 2012. Field Experiments: Design, Analysis, and Interpretation. New York, NY: W. W. Norton &amp; Company. Glennerster, Rachel, and Kudzai Takavarasha. 2013. Running Randomized Evaluations: A Practical Guide. Princeton: Princeton University Press. Morgan, Stephen L., and Christopher Winship. 2007. Counterfactuals and Causal Inference: Methods and Principles for Social Research. Cambridge Univ Press. "]]
